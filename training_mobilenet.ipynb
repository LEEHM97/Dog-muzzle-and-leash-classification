{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import scipy as sp\n",
    "import PIL\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow.keras import models, layers, Model\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, ZeroPadding2D\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import MobileNetV2, imagenet_utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size: 한번에 forward & Backword 하는 샘플의 수\n",
    "batch_size = 32\n",
    "\n",
    "# Training 수\n",
    "epochs = 100\n",
    "\n",
    "# Weight 조절 parameter\n",
    "LearningRate = 1e-3 # 0.001\n",
    "Decay = 1e-6\n",
    "\n",
    "img_width = 225\n",
    "img_height = 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CurrentDirectory = '../dataset/'\n",
    "\n",
    "train_directory = CurrentDirectory + 'train/'\n",
    "test_directory  = CurrentDirectory + 'test/'\n",
    "model_directory = CurrentDirectory + '../MODEL/'\n",
    "tensorboard_directory = CurrentDirectory + '../Tensorboard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gmlkd\\miniforge3\\envs\\tensor2.9\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 모델 Return\n",
    "MobileNetV2Model= tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(img_width,img_height,3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    pooling=None)\n",
    "x = GlobalAveragePooling2D()(MobileNetV2Model.output)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "predictions = Dense(3, activation='sigmoid')(x)\n",
    "# Input ~ Output 연결해주기\n",
    "DeepLearning = Model(inputs=MobileNetV2Model.input, outputs=predictions)\n",
    "\n",
    "# learning parameter를 더하여 최종 model compile\n",
    "DeepLearning.compile(optimizer=\n",
    "         SGD(lr=LearningRate, decay=Decay, momentum=0.9, nesterov=True), \n",
    "         loss='categorical_crossentropy',\n",
    "         metrics=['acc']\n",
    ") # 나이를, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online-augmentation 적용 Generator\n",
    "# 1. 이미지를 전부다 불러서 램 (메모리)에 올릴 수 없기 때문\n",
    "# 2. 이미지는 Augmentation을 해주는게 좋아서\n",
    "\n",
    "DATAGEN_TRAIN = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    data_format=\"channels_last\",\n",
    "    validation_split=0.10) # Train / Validation\n",
    "\n",
    "# Online-augmentation 비적용 Generator (Test용)\n",
    "DATAGEN_TEST = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    data_format=\"channels_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 734 images belonging to 3 classes.\n",
      "Found 80 images belonging to 3 classes.\n",
      "Found 91 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generator의 instance 생성 (Train)\n",
    "TRAIN_GENERATOR = DATAGEN_TRAIN.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode= \"categorical\",\n",
    "    subset = \"training\")\n",
    "\n",
    "VALID_GENERATOR = DATAGEN_TRAIN.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset = \"validation\")\n",
    "\n",
    "# Generator의 instance 생성 (Test)\n",
    "TEST_GENERATOR = DATAGEN_TEST.flow_from_directory(\n",
    "    test_directory,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_GENERATOR.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call-back 함수\n",
    "\n",
    "# CheckPoint: Epoch 마다 validation 성능을 검증하여, best performance 일 경우 저장\n",
    "CP = ModelCheckpoint(filepath=model_directory+'MobileNetV2-{epoch:03d}-{val_loss:.4f}-{val_acc:.4f}.hdf5',\n",
    "            monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# 학습과정 진행사항 확인\n",
    "TB = TensorBoard(log_dir=tensorboard_directory, write_graph=True, write_images=True)\n",
    "\n",
    "# Learning Rate 줄여나가기\n",
    "LR = ReduceLROnPlateau(monitor='val_loss',factor=0.8,patience=3, verbose=1, min_lr=1e-8)\n",
    "\n",
    "CALLBACK = [CP, TB, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gmlkd\\AppData\\Local\\Temp\\ipykernel_2596\\3034265299.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  DeepLearning.fit_generator(\n",
      "c:\\Users\\gmlkd\\miniforge3\\envs\\tensor2.9\\lib\\site-packages\\keras\\preprocessing\\image.py:1663: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "c:\\Users\\gmlkd\\miniforge3\\envs\\tensor2.9\\lib\\site-packages\\keras\\preprocessing\\image.py:1671: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0732 - acc: 0.4796\n",
      "Epoch 1: val_acc improved from -inf to 0.55000, saving model to ../dataset/../MODEL\\MobileNetV2-001-0.9755-0.5500.hdf5\n",
      "23/23 [==============================] - 16s 449ms/step - loss: 1.0732 - acc: 0.4796 - val_loss: 0.9755 - val_acc: 0.5500 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6528 - acc: 0.7657\n",
      "Epoch 2: val_acc did not improve from 0.55000\n",
      "23/23 [==============================] - 9s 388ms/step - loss: 0.6528 - acc: 0.7657 - val_loss: 1.2258 - val_acc: 0.4875 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4809 - acc: 0.8229\n",
      "Epoch 3: val_acc did not improve from 0.55000\n",
      "23/23 [==============================] - 9s 398ms/step - loss: 0.4809 - acc: 0.8229 - val_loss: 1.0802 - val_acc: 0.5375 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3912 - acc: 0.8597\n",
      "Epoch 4: val_acc did not improve from 0.55000\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "23/23 [==============================] - 9s 373ms/step - loss: 0.3912 - acc: 0.8597 - val_loss: 1.5025 - val_acc: 0.4500 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3097 - acc: 0.8965\n",
      "Epoch 5: val_acc did not improve from 0.55000\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.3097 - acc: 0.8965 - val_loss: 1.4017 - val_acc: 0.5125 - lr: 8.0000e-04\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2737 - acc: 0.9074\n",
      "Epoch 6: val_acc did not improve from 0.55000\n",
      "23/23 [==============================] - 9s 380ms/step - loss: 0.2737 - acc: 0.9074 - val_loss: 1.4523 - val_acc: 0.5125 - lr: 8.0000e-04\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2316 - acc: 0.9237\n",
      "Epoch 7: val_acc did not improve from 0.55000\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "23/23 [==============================] - 9s 377ms/step - loss: 0.2316 - acc: 0.9237 - val_loss: 1.6728 - val_acc: 0.4875 - lr: 8.0000e-04\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2214 - acc: 0.9223\n",
      "Epoch 8: val_acc improved from 0.55000 to 0.58750, saving model to ../dataset/../MODEL\\MobileNetV2-008-1.2767-0.5875.hdf5\n",
      "23/23 [==============================] - 9s 385ms/step - loss: 0.2214 - acc: 0.9223 - val_loss: 1.2767 - val_acc: 0.5875 - lr: 6.4000e-04\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1775 - acc: 0.9523\n",
      "Epoch 9: val_acc did not improve from 0.58750\n",
      "23/23 [==============================] - 9s 379ms/step - loss: 0.1775 - acc: 0.9523 - val_loss: 1.4403 - val_acc: 0.5500 - lr: 6.4000e-04\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1690 - acc: 0.9401\n",
      "Epoch 10: val_acc did not improve from 0.58750\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "23/23 [==============================] - 9s 374ms/step - loss: 0.1690 - acc: 0.9401 - val_loss: 1.4737 - val_acc: 0.5375 - lr: 6.4000e-04\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1684 - acc: 0.9346\n",
      "Epoch 11: val_acc improved from 0.58750 to 0.60000, saving model to ../dataset/../MODEL\\MobileNetV2-011-1.2630-0.6000.hdf5\n",
      "23/23 [==============================] - 9s 385ms/step - loss: 0.1684 - acc: 0.9346 - val_loss: 1.2630 - val_acc: 0.6000 - lr: 5.1200e-04\n",
      "Epoch 12/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1687 - acc: 0.9387\n",
      "Epoch 12: val_acc did not improve from 0.60000\n",
      "23/23 [==============================] - 9s 397ms/step - loss: 0.1687 - acc: 0.9387 - val_loss: 1.4715 - val_acc: 0.5500 - lr: 5.1200e-04\n",
      "Epoch 13/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1545 - acc: 0.9482\n",
      "Epoch 13: val_acc did not improve from 0.60000\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "23/23 [==============================] - 9s 376ms/step - loss: 0.1545 - acc: 0.9482 - val_loss: 1.2929 - val_acc: 0.5750 - lr: 5.1200e-04\n",
      "Epoch 14/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1302 - acc: 0.9673\n",
      "Epoch 14: val_acc did not improve from 0.60000\n",
      "23/23 [==============================] - 9s 379ms/step - loss: 0.1302 - acc: 0.9673 - val_loss: 1.2156 - val_acc: 0.5625 - lr: 4.0960e-04\n",
      "Epoch 15/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1251 - acc: 0.9578\n",
      "Epoch 15: val_acc improved from 0.60000 to 0.65000, saving model to ../dataset/../MODEL\\MobileNetV2-015-1.0685-0.6500.hdf5\n",
      "23/23 [==============================] - 9s 390ms/step - loss: 0.1251 - acc: 0.9578 - val_loss: 1.0685 - val_acc: 0.6500 - lr: 4.0960e-04\n",
      "Epoch 16/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1062 - acc: 0.9728\n",
      "Epoch 16: val_acc did not improve from 0.65000\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "23/23 [==============================] - 9s 378ms/step - loss: 0.1062 - acc: 0.9728 - val_loss: 1.0483 - val_acc: 0.6250 - lr: 4.0960e-04\n",
      "Epoch 17/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1056 - acc: 0.9728\n",
      "Epoch 17: val_acc improved from 0.65000 to 0.66250, saving model to ../dataset/../MODEL\\MobileNetV2-017-1.0432-0.6625.hdf5\n",
      "23/23 [==============================] - 9s 385ms/step - loss: 0.1056 - acc: 0.9728 - val_loss: 1.0432 - val_acc: 0.6625 - lr: 3.2768e-04\n",
      "Epoch 18/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1139 - acc: 0.9700\n",
      "Epoch 18: val_acc did not improve from 0.66250\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.1139 - acc: 0.9700 - val_loss: 1.1288 - val_acc: 0.5625 - lr: 3.2768e-04\n",
      "Epoch 19/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0897 - acc: 0.9714\n",
      "Epoch 19: val_acc improved from 0.66250 to 0.67500, saving model to ../dataset/../MODEL\\MobileNetV2-019-0.9074-0.6750.hdf5\n",
      "23/23 [==============================] - 9s 388ms/step - loss: 0.0897 - acc: 0.9714 - val_loss: 0.9074 - val_acc: 0.6750 - lr: 3.2768e-04\n",
      "Epoch 20/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1037 - acc: 0.9619\n",
      "Epoch 20: val_acc improved from 0.67500 to 0.70000, saving model to ../dataset/../MODEL\\MobileNetV2-020-0.9373-0.7000.hdf5\n",
      "23/23 [==============================] - 9s 389ms/step - loss: 0.1037 - acc: 0.9619 - val_loss: 0.9373 - val_acc: 0.7000 - lr: 3.2768e-04\n",
      "Epoch 21/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0978 - acc: 0.9728\n",
      "Epoch 21: val_acc did not improve from 0.70000\n",
      "23/23 [==============================] - 9s 377ms/step - loss: 0.0978 - acc: 0.9728 - val_loss: 0.9052 - val_acc: 0.6500 - lr: 3.2768e-04\n",
      "Epoch 22/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0910 - acc: 0.9673\n",
      "Epoch 22: val_acc did not improve from 0.70000\n",
      "23/23 [==============================] - 9s 396ms/step - loss: 0.0910 - acc: 0.9673 - val_loss: 0.9286 - val_acc: 0.6875 - lr: 3.2768e-04\n",
      "Epoch 23/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0790 - acc: 0.9823\n",
      "Epoch 23: val_acc did not improve from 0.70000\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0790 - acc: 0.9823 - val_loss: 0.9407 - val_acc: 0.6625 - lr: 3.2768e-04\n",
      "Epoch 24/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0770 - acc: 0.9796\n",
      "Epoch 24: val_acc did not improve from 0.70000\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "23/23 [==============================] - 9s 383ms/step - loss: 0.0770 - acc: 0.9796 - val_loss: 1.0935 - val_acc: 0.6500 - lr: 3.2768e-04\n",
      "Epoch 25/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0763 - acc: 0.9755\n",
      "Epoch 25: val_acc did not improve from 0.70000\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0763 - acc: 0.9755 - val_loss: 0.8673 - val_acc: 0.6375 - lr: 2.6214e-04\n",
      "Epoch 26/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0614 - acc: 0.9837\n",
      "Epoch 26: val_acc did not improve from 0.70000\n",
      "23/23 [==============================] - 9s 390ms/step - loss: 0.0614 - acc: 0.9837 - val_loss: 0.9597 - val_acc: 0.6875 - lr: 2.6214e-04\n",
      "Epoch 27/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0741 - acc: 0.9755\n",
      "Epoch 27: val_acc improved from 0.70000 to 0.75000, saving model to ../dataset/../MODEL\\MobileNetV2-027-0.6491-0.7500.hdf5\n",
      "23/23 [==============================] - 9s 392ms/step - loss: 0.0741 - acc: 0.9755 - val_loss: 0.6491 - val_acc: 0.7500 - lr: 2.6214e-04\n",
      "Epoch 28/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0736 - acc: 0.9768\n",
      "Epoch 28: val_acc did not improve from 0.75000\n",
      "23/23 [==============================] - 9s 380ms/step - loss: 0.0736 - acc: 0.9768 - val_loss: 0.8296 - val_acc: 0.6625 - lr: 2.6214e-04\n",
      "Epoch 29/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0691 - acc: 0.9837\n",
      "Epoch 29: val_acc did not improve from 0.75000\n",
      "23/23 [==============================] - 9s 378ms/step - loss: 0.0691 - acc: 0.9837 - val_loss: 0.9101 - val_acc: 0.6750 - lr: 2.6214e-04\n",
      "Epoch 30/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0551 - acc: 0.9932\n",
      "Epoch 30: val_acc did not improve from 0.75000\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "23/23 [==============================] - 9s 385ms/step - loss: 0.0551 - acc: 0.9932 - val_loss: 0.8987 - val_acc: 0.7375 - lr: 2.6214e-04\n",
      "Epoch 31/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0576 - acc: 0.9796\n",
      "Epoch 31: val_acc did not improve from 0.75000\n",
      "23/23 [==============================] - 9s 382ms/step - loss: 0.0576 - acc: 0.9796 - val_loss: 0.9752 - val_acc: 0.6875 - lr: 2.0972e-04\n",
      "Epoch 32/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0819 - acc: 0.9728\n",
      "Epoch 32: val_acc did not improve from 0.75000\n",
      "23/23 [==============================] - 9s 383ms/step - loss: 0.0819 - acc: 0.9728 - val_loss: 0.7851 - val_acc: 0.7000 - lr: 2.0972e-04\n",
      "Epoch 33/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0707 - acc: 0.9755\n",
      "Epoch 33: val_acc did not improve from 0.75000\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0707 - acc: 0.9755 - val_loss: 0.8029 - val_acc: 0.7125 - lr: 2.0972e-04\n",
      "Epoch 34/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0644 - acc: 0.9768\n",
      "Epoch 34: val_acc improved from 0.75000 to 0.77500, saving model to ../dataset/../MODEL\\MobileNetV2-034-0.6533-0.7750.hdf5\n",
      "23/23 [==============================] - 9s 389ms/step - loss: 0.0644 - acc: 0.9768 - val_loss: 0.6533 - val_acc: 0.7750 - lr: 1.6777e-04\n",
      "Epoch 35/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0529 - acc: 0.9891\n",
      "Epoch 35: val_acc did not improve from 0.77500\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0529 - acc: 0.9891 - val_loss: 0.7064 - val_acc: 0.7500 - lr: 1.6777e-04\n",
      "Epoch 36/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0501 - acc: 0.9877\n",
      "Epoch 36: val_acc improved from 0.77500 to 0.81250, saving model to ../dataset/../MODEL\\MobileNetV2-036-0.6570-0.8125.hdf5\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "23/23 [==============================] - 9s 392ms/step - loss: 0.0501 - acc: 0.9877 - val_loss: 0.6570 - val_acc: 0.8125 - lr: 1.6777e-04\n",
      "Epoch 37/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0538 - acc: 0.9809\n",
      "Epoch 37: val_acc did not improve from 0.81250\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0538 - acc: 0.9809 - val_loss: 0.6277 - val_acc: 0.7875 - lr: 1.3422e-04\n",
      "Epoch 38/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0593 - acc: 0.9809\n",
      "Epoch 38: val_acc did not improve from 0.81250\n",
      "23/23 [==============================] - 9s 379ms/step - loss: 0.0593 - acc: 0.9809 - val_loss: 0.6006 - val_acc: 0.7500 - lr: 1.3422e-04\n",
      "Epoch 39/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0701 - acc: 0.9782\n",
      "Epoch 39: val_acc did not improve from 0.81250\n",
      "23/23 [==============================] - 9s 383ms/step - loss: 0.0701 - acc: 0.9782 - val_loss: 0.7585 - val_acc: 0.7500 - lr: 1.3422e-04\n",
      "Epoch 40/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0548 - acc: 0.9850\n",
      "Epoch 40: val_acc did not improve from 0.81250\n",
      "23/23 [==============================] - 9s 383ms/step - loss: 0.0548 - acc: 0.9850 - val_loss: 0.6211 - val_acc: 0.8000 - lr: 1.3422e-04\n",
      "Epoch 41/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0533 - acc: 0.9864\n",
      "Epoch 41: val_acc did not improve from 0.81250\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0533 - acc: 0.9864 - val_loss: 0.4927 - val_acc: 0.8125 - lr: 1.3422e-04\n",
      "Epoch 42/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0730 - acc: 0.9796\n",
      "Epoch 42: val_acc did not improve from 0.81250\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0730 - acc: 0.9796 - val_loss: 0.6891 - val_acc: 0.7375 - lr: 1.3422e-04\n",
      "Epoch 43/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0586 - acc: 0.9850\n",
      "Epoch 43: val_acc did not improve from 0.81250\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0586 - acc: 0.9850 - val_loss: 0.5355 - val_acc: 0.8125 - lr: 1.3422e-04\n",
      "Epoch 44/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0473 - acc: 0.9891\n",
      "Epoch 44: val_acc improved from 0.81250 to 0.83750, saving model to ../dataset/../MODEL\\MobileNetV2-044-0.4682-0.8375.hdf5\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0473 - acc: 0.9891 - val_loss: 0.4682 - val_acc: 0.8375 - lr: 1.3422e-04\n",
      "Epoch 45/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0560 - acc: 0.9850\n",
      "Epoch 45: val_acc did not improve from 0.83750\n",
      "23/23 [==============================] - 9s 384ms/step - loss: 0.0560 - acc: 0.9850 - val_loss: 0.5908 - val_acc: 0.8000 - lr: 1.3422e-04\n",
      "Epoch 46/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0615 - acc: 0.9809\n",
      "Epoch 46: val_acc did not improve from 0.83750\n",
      "23/23 [==============================] - 9s 382ms/step - loss: 0.0615 - acc: 0.9809 - val_loss: 0.5991 - val_acc: 0.8000 - lr: 1.3422e-04\n",
      "Epoch 47/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0649 - acc: 0.9782\n",
      "Epoch 47: val_acc did not improve from 0.83750\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "23/23 [==============================] - 9s 383ms/step - loss: 0.0649 - acc: 0.9782 - val_loss: 0.5925 - val_acc: 0.8000 - lr: 1.3422e-04\n",
      "Epoch 48/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0446 - acc: 0.9891\n",
      "Epoch 48: val_acc did not improve from 0.83750\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0446 - acc: 0.9891 - val_loss: 0.5698 - val_acc: 0.8125 - lr: 1.0737e-04\n",
      "Epoch 49/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0450 - acc: 0.9891\n",
      "Epoch 49: val_acc did not improve from 0.83750\n",
      "23/23 [==============================] - 9s 384ms/step - loss: 0.0450 - acc: 0.9891 - val_loss: 0.5959 - val_acc: 0.7750 - lr: 1.0737e-04\n",
      "Epoch 50/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0384 - acc: 0.9905\n",
      "Epoch 50: val_acc improved from 0.83750 to 0.85000, saving model to ../dataset/../MODEL\\MobileNetV2-050-0.5494-0.8500.hdf5\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
      "23/23 [==============================] - 9s 387ms/step - loss: 0.0384 - acc: 0.9905 - val_loss: 0.5494 - val_acc: 0.8500 - lr: 1.0737e-04\n",
      "Epoch 51/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0594 - acc: 0.9809\n",
      "Epoch 51: val_acc did not improve from 0.85000\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0594 - acc: 0.9809 - val_loss: 0.6211 - val_acc: 0.7875 - lr: 8.5899e-05\n",
      "Epoch 52/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0555 - acc: 0.9837\n",
      "Epoch 52: val_acc did not improve from 0.85000\n",
      "23/23 [==============================] - 9s 384ms/step - loss: 0.0555 - acc: 0.9837 - val_loss: 0.6128 - val_acc: 0.8375 - lr: 8.5899e-05\n",
      "Epoch 53/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0566 - acc: 0.9837\n",
      "Epoch 53: val_acc did not improve from 0.85000\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.871948717162013e-05.\n",
      "23/23 [==============================] - 9s 383ms/step - loss: 0.0566 - acc: 0.9837 - val_loss: 0.4883 - val_acc: 0.8000 - lr: 8.5899e-05\n",
      "Epoch 54/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0554 - acc: 0.9877\n",
      "Epoch 54: val_acc did not improve from 0.85000\n",
      "23/23 [==============================] - 9s 380ms/step - loss: 0.0554 - acc: 0.9877 - val_loss: 0.4618 - val_acc: 0.8500 - lr: 6.8719e-05\n",
      "Epoch 55/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0490 - acc: 0.9905\n",
      "Epoch 55: val_acc did not improve from 0.85000\n",
      "23/23 [==============================] - 9s 382ms/step - loss: 0.0490 - acc: 0.9905 - val_loss: 0.5719 - val_acc: 0.8000 - lr: 6.8719e-05\n",
      "Epoch 56/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0433 - acc: 0.9864\n",
      "Epoch 56: val_acc did not improve from 0.85000\n",
      "23/23 [==============================] - 9s 383ms/step - loss: 0.0433 - acc: 0.9864 - val_loss: 0.4685 - val_acc: 0.8375 - lr: 6.8719e-05\n",
      "Epoch 57/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0407 - acc: 0.9918\n",
      "Epoch 57: val_acc did not improve from 0.85000\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 5.497558740898967e-05.\n",
      "23/23 [==============================] - 9s 382ms/step - loss: 0.0407 - acc: 0.9918 - val_loss: 0.5801 - val_acc: 0.8375 - lr: 6.8719e-05\n",
      "Epoch 58/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0624 - acc: 0.9837\n",
      "Epoch 58: val_acc did not improve from 0.85000\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0624 - acc: 0.9837 - val_loss: 0.4439 - val_acc: 0.8500 - lr: 5.4976e-05\n",
      "Epoch 59/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0513 - acc: 0.9864\n",
      "Epoch 59: val_acc improved from 0.85000 to 0.86250, saving model to ../dataset/../MODEL\\MobileNetV2-059-0.4448-0.8625.hdf5\n",
      "23/23 [==============================] - 9s 389ms/step - loss: 0.0513 - acc: 0.9864 - val_loss: 0.4448 - val_acc: 0.8625 - lr: 5.4976e-05\n",
      "Epoch 60/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0485 - acc: 0.9877\n",
      "Epoch 60: val_acc did not improve from 0.86250\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0485 - acc: 0.9877 - val_loss: 0.5468 - val_acc: 0.8500 - lr: 5.4976e-05\n",
      "Epoch 61/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0435 - acc: 0.9905\n",
      "Epoch 61: val_acc improved from 0.86250 to 0.88750, saving model to ../dataset/../MODEL\\MobileNetV2-061-0.4123-0.8875.hdf5\n",
      "23/23 [==============================] - 9s 398ms/step - loss: 0.0435 - acc: 0.9905 - val_loss: 0.4123 - val_acc: 0.8875 - lr: 5.4976e-05\n",
      "Epoch 62/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0616 - acc: 0.9782\n",
      "Epoch 62: val_acc did not improve from 0.88750\n",
      "23/23 [==============================] - 9s 383ms/step - loss: 0.0616 - acc: 0.9782 - val_loss: 0.5421 - val_acc: 0.8375 - lr: 5.4976e-05\n",
      "Epoch 63/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0631 - acc: 0.9809\n",
      "Epoch 63: val_acc did not improve from 0.88750\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0631 - acc: 0.9809 - val_loss: 0.4662 - val_acc: 0.8375 - lr: 5.4976e-05\n",
      "Epoch 64/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0344 - acc: 0.9959\n",
      "Epoch 64: val_acc did not improve from 0.88750\n",
      "23/23 [==============================] - 9s 380ms/step - loss: 0.0344 - acc: 0.9959 - val_loss: 0.3585 - val_acc: 0.8875 - lr: 5.4976e-05\n",
      "Epoch 65/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0542 - acc: 0.9891\n",
      "Epoch 65: val_acc did not improve from 0.88750\n",
      "23/23 [==============================] - 9s 384ms/step - loss: 0.0542 - acc: 0.9891 - val_loss: 0.4852 - val_acc: 0.8625 - lr: 5.4976e-05\n",
      "Epoch 66/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0405 - acc: 0.9864\n",
      "Epoch 66: val_acc did not improve from 0.88750\n",
      "23/23 [==============================] - 9s 383ms/step - loss: 0.0405 - acc: 0.9864 - val_loss: 0.4758 - val_acc: 0.8375 - lr: 5.4976e-05\n",
      "Epoch 67/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0418 - acc: 0.9891\n",
      "Epoch 67: val_acc did not improve from 0.88750\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 4.398046876303852e-05.\n",
      "23/23 [==============================] - 9s 379ms/step - loss: 0.0418 - acc: 0.9891 - val_loss: 0.5464 - val_acc: 0.8375 - lr: 5.4976e-05\n",
      "Epoch 68/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0623 - acc: 0.9837\n",
      "Epoch 68: val_acc did not improve from 0.88750\n",
      "23/23 [==============================] - 9s 385ms/step - loss: 0.0623 - acc: 0.9837 - val_loss: 0.5287 - val_acc: 0.8500 - lr: 4.3980e-05\n",
      "Epoch 69/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0658 - acc: 0.9837\n",
      "Epoch 69: val_acc did not improve from 0.88750\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0658 - acc: 0.9837 - val_loss: 0.3734 - val_acc: 0.8625 - lr: 4.3980e-05\n",
      "Epoch 70/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0498 - acc: 0.9877\n",
      "Epoch 70: val_acc did not improve from 0.88750\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 3.518437442835421e-05.\n",
      "23/23 [==============================] - 9s 380ms/step - loss: 0.0498 - acc: 0.9877 - val_loss: 0.4820 - val_acc: 0.8625 - lr: 4.3980e-05\n",
      "Epoch 71/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0475 - acc: 0.9864\n",
      "Epoch 71: val_acc did not improve from 0.88750\n",
      "23/23 [==============================] - 9s 379ms/step - loss: 0.0475 - acc: 0.9864 - val_loss: 0.5859 - val_acc: 0.8500 - lr: 3.5184e-05\n",
      "Epoch 72/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0437 - acc: 0.9864\n",
      "Epoch 72: val_acc did not improve from 0.88750\n",
      "23/23 [==============================] - 9s 384ms/step - loss: 0.0437 - acc: 0.9864 - val_loss: 0.5517 - val_acc: 0.8375 - lr: 3.5184e-05\n",
      "Epoch 73/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0427 - acc: 0.9891\n",
      "Epoch 73: val_acc did not improve from 0.88750\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 2.8147498960606756e-05.\n",
      "23/23 [==============================] - 9s 376ms/step - loss: 0.0427 - acc: 0.9891 - val_loss: 0.6096 - val_acc: 0.8250 - lr: 3.5184e-05\n",
      "Epoch 74/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0435 - acc: 0.9864\n",
      "Epoch 74: val_acc did not improve from 0.88750\n",
      "23/23 [==============================] - 9s 382ms/step - loss: 0.0435 - acc: 0.9864 - val_loss: 0.5012 - val_acc: 0.8500 - lr: 2.8147e-05\n",
      "Epoch 75/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0441 - acc: 0.9905\n",
      "Epoch 75: val_acc did not improve from 0.88750\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0441 - acc: 0.9905 - val_loss: 0.4931 - val_acc: 0.8625 - lr: 2.8147e-05\n",
      "Epoch 76/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0425 - acc: 0.9905\n",
      "Epoch 76: val_acc did not improve from 0.88750\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 2.25179988774471e-05.\n",
      "23/23 [==============================] - 9s 378ms/step - loss: 0.0425 - acc: 0.9905 - val_loss: 0.4259 - val_acc: 0.8500 - lr: 2.8147e-05\n",
      "Epoch 77/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0558 - acc: 0.9809\n",
      "Epoch 77: val_acc did not improve from 0.88750\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0558 - acc: 0.9809 - val_loss: 0.4320 - val_acc: 0.8875 - lr: 2.2518e-05\n",
      "Epoch 78/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0458 - acc: 0.9877\n",
      "Epoch 78: val_acc improved from 0.88750 to 0.91250, saving model to ../dataset/../MODEL\\MobileNetV2-078-0.3642-0.9125.hdf5\n",
      "23/23 [==============================] - 9s 391ms/step - loss: 0.0458 - acc: 0.9877 - val_loss: 0.3642 - val_acc: 0.9125 - lr: 2.2518e-05\n",
      "Epoch 79/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0595 - acc: 0.9837\n",
      "Epoch 79: val_acc did not improve from 0.91250\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 1.8014399392995985e-05.\n",
      "23/23 [==============================] - 9s 382ms/step - loss: 0.0595 - acc: 0.9837 - val_loss: 0.4462 - val_acc: 0.9000 - lr: 2.2518e-05\n",
      "Epoch 80/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0442 - acc: 0.9891\n",
      "Epoch 80: val_acc did not improve from 0.91250\n",
      "23/23 [==============================] - 9s 383ms/step - loss: 0.0442 - acc: 0.9891 - val_loss: 0.3274 - val_acc: 0.8375 - lr: 1.8014e-05\n",
      "Epoch 81/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0424 - acc: 0.9905\n",
      "Epoch 81: val_acc did not improve from 0.91250\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0424 - acc: 0.9905 - val_loss: 0.3603 - val_acc: 0.9000 - lr: 1.8014e-05\n",
      "Epoch 82/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0448 - acc: 0.9877\n",
      "Epoch 82: val_acc did not improve from 0.91250\n",
      "23/23 [==============================] - 9s 379ms/step - loss: 0.0448 - acc: 0.9877 - val_loss: 0.4235 - val_acc: 0.8875 - lr: 1.8014e-05\n",
      "Epoch 83/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0403 - acc: 0.9891\n",
      "Epoch 83: val_acc did not improve from 0.91250\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.4411519805435093e-05.\n",
      "23/23 [==============================] - 9s 383ms/step - loss: 0.0403 - acc: 0.9891 - val_loss: 0.4086 - val_acc: 0.8625 - lr: 1.8014e-05\n",
      "Epoch 84/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0529 - acc: 0.9809\n",
      "Epoch 84: val_acc did not improve from 0.91250\n",
      "23/23 [==============================] - 9s 385ms/step - loss: 0.0529 - acc: 0.9809 - val_loss: 0.4746 - val_acc: 0.8500 - lr: 1.4412e-05\n",
      "Epoch 85/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0327 - acc: 0.9959\n",
      "Epoch 85: val_acc did not improve from 0.91250\n",
      "23/23 [==============================] - 9s 384ms/step - loss: 0.0327 - acc: 0.9959 - val_loss: 0.3605 - val_acc: 0.8875 - lr: 1.4412e-05\n",
      "Epoch 86/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0398 - acc: 0.9932\n",
      "Epoch 86: val_acc did not improve from 0.91250\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 1.1529216135386379e-05.\n",
      "23/23 [==============================] - 9s 382ms/step - loss: 0.0398 - acc: 0.9932 - val_loss: 0.4535 - val_acc: 0.8875 - lr: 1.4412e-05\n",
      "Epoch 87/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0428 - acc: 0.9891\n",
      "Epoch 87: val_acc did not improve from 0.91250\n",
      "23/23 [==============================] - 9s 378ms/step - loss: 0.0428 - acc: 0.9891 - val_loss: 0.4266 - val_acc: 0.9000 - lr: 1.1529e-05\n",
      "Epoch 88/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0569 - acc: 0.9837\n",
      "Epoch 88: val_acc did not improve from 0.91250\n",
      "23/23 [==============================] - 9s 379ms/step - loss: 0.0569 - acc: 0.9837 - val_loss: 0.3973 - val_acc: 0.8750 - lr: 1.1529e-05\n",
      "Epoch 89/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0501 - acc: 0.9823\n",
      "Epoch 89: val_acc improved from 0.91250 to 0.92500, saving model to ../dataset/../MODEL\\MobileNetV2-089-0.3565-0.9250.hdf5\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 9.223372762789951e-06.\n",
      "23/23 [==============================] - 9s 387ms/step - loss: 0.0501 - acc: 0.9823 - val_loss: 0.3565 - val_acc: 0.9250 - lr: 1.1529e-05\n",
      "Epoch 90/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0395 - acc: 0.9891\n",
      "Epoch 90: val_acc did not improve from 0.92500\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0395 - acc: 0.9891 - val_loss: 0.3844 - val_acc: 0.8625 - lr: 9.2234e-06\n",
      "Epoch 91/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0318 - acc: 0.9959\n",
      "Epoch 91: val_acc did not improve from 0.92500\n",
      "23/23 [==============================] - 9s 380ms/step - loss: 0.0318 - acc: 0.9959 - val_loss: 0.4781 - val_acc: 0.8625 - lr: 9.2234e-06\n",
      "Epoch 92/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0382 - acc: 0.9932\n",
      "Epoch 92: val_acc did not improve from 0.92500\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 7.378698501270265e-06.\n",
      "23/23 [==============================] - 9s 382ms/step - loss: 0.0382 - acc: 0.9932 - val_loss: 0.4396 - val_acc: 0.8750 - lr: 9.2234e-06\n",
      "Epoch 93/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0448 - acc: 0.9905\n",
      "Epoch 93: val_acc did not improve from 0.92500\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0448 - acc: 0.9905 - val_loss: 0.3901 - val_acc: 0.8875 - lr: 7.3787e-06\n",
      "Epoch 94/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0411 - acc: 0.9905\n",
      "Epoch 94: val_acc did not improve from 0.92500\n",
      "23/23 [==============================] - 9s 383ms/step - loss: 0.0411 - acc: 0.9905 - val_loss: 0.4391 - val_acc: 0.8875 - lr: 7.3787e-06\n",
      "Epoch 95/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0425 - acc: 0.9877\n",
      "Epoch 95: val_acc did not improve from 0.92500\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 5.902958946535364e-06.\n",
      "23/23 [==============================] - 9s 384ms/step - loss: 0.0425 - acc: 0.9877 - val_loss: 0.4228 - val_acc: 0.8750 - lr: 7.3787e-06\n",
      "Epoch 96/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0484 - acc: 0.9864\n",
      "Epoch 96: val_acc did not improve from 0.92500\n",
      "23/23 [==============================] - 9s 382ms/step - loss: 0.0484 - acc: 0.9864 - val_loss: 0.4507 - val_acc: 0.8750 - lr: 5.9030e-06\n",
      "Epoch 97/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0487 - acc: 0.9823\n",
      "Epoch 97: val_acc did not improve from 0.92500\n",
      "23/23 [==============================] - 9s 381ms/step - loss: 0.0487 - acc: 0.9823 - val_loss: 0.4868 - val_acc: 0.8750 - lr: 5.9030e-06\n",
      "Epoch 98/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0495 - acc: 0.9837\n",
      "Epoch 98: val_acc did not improve from 0.92500\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.7223671572282915e-06.\n",
      "23/23 [==============================] - 9s 380ms/step - loss: 0.0495 - acc: 0.9837 - val_loss: 0.4055 - val_acc: 0.9000 - lr: 5.9030e-06\n",
      "Epoch 99/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0433 - acc: 0.9877\n",
      "Epoch 99: val_acc did not improve from 0.92500\n",
      "23/23 [==============================] - 9s 393ms/step - loss: 0.0433 - acc: 0.9877 - val_loss: 0.4127 - val_acc: 0.8500 - lr: 4.7224e-06\n",
      "Epoch 100/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0431 - acc: 0.9891\n",
      "Epoch 100: val_acc did not improve from 0.92500\n",
      "23/23 [==============================] - 9s 383ms/step - loss: 0.0431 - acc: 0.9891 - val_loss: 0.5441 - val_acc: 0.8375 - lr: 4.7224e-06\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/device:GPU:0\"):\n",
    "        history = DeepLearning.fit_generator(\n",
    "                TRAIN_GENERATOR,\n",
    "                # steps_per_epoch=TRAIN_GENERATOR.n / batch_size,\n",
    "                epochs=epochs,\n",
    "                callbacks=CALLBACK,\n",
    "                shuffle=True, # Training에 패턴이 존재하면 overfit이 잘 되기 때문에, Shuffle 사용해야함. 단 test에는 절대 X\n",
    "                validation_data=VALID_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepLearning.load_weights(model_directory+'MobileNetV2-089-0.3565-0.9250.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gmlkd\\miniforge3\\envs\\tensor2.9\\lib\\site-packages\\keras\\preprocessing\\image.py:1663: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "c:\\Users\\gmlkd\\miniforge3\\envs\\tensor2.9\\lib\\site-packages\\keras\\preprocessing\\image.py:1671: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 180ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = DeepLearning.predict(TEST_GENERATOR, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.891443</td>\n",
       "      <td>0.040257</td>\n",
       "      <td>0.607437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998034</td>\n",
       "      <td>0.061743</td>\n",
       "      <td>0.007353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.970399</td>\n",
       "      <td>0.091075</td>\n",
       "      <td>0.215295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.921385</td>\n",
       "      <td>0.041969</td>\n",
       "      <td>0.784807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.988280</td>\n",
       "      <td>0.049735</td>\n",
       "      <td>0.155549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.081579</td>\n",
       "      <td>0.141124</td>\n",
       "      <td>0.967623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.129755</td>\n",
       "      <td>0.214087</td>\n",
       "      <td>0.954455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.174862</td>\n",
       "      <td>0.133242</td>\n",
       "      <td>0.982040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.107839</td>\n",
       "      <td>0.396587</td>\n",
       "      <td>0.878742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.393867</td>\n",
       "      <td>0.285136</td>\n",
       "      <td>0.844094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2\n",
       "0   0.891443  0.040257  0.607437\n",
       "1   0.998034  0.061743  0.007353\n",
       "2   0.970399  0.091075  0.215295\n",
       "3   0.921385  0.041969  0.784807\n",
       "4   0.988280  0.049735  0.155549\n",
       "..       ...       ...       ...\n",
       "86  0.081579  0.141124  0.967623\n",
       "87  0.129755  0.214087  0.954455\n",
       "88  0.174862  0.133242  0.982040\n",
       "89  0.107839  0.396587  0.878742\n",
       "90  0.393867  0.285136  0.844094\n",
       "\n",
       "[91 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result = pd.DataFrame(prediction)\n",
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.089724</td>\n",
       "      <td>0.006591</td>\n",
       "      <td>0.992500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.093017</td>\n",
       "      <td>0.103669</td>\n",
       "      <td>0.959554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.108823</td>\n",
       "      <td>0.043440</td>\n",
       "      <td>0.996305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.292536</td>\n",
       "      <td>0.638150</td>\n",
       "      <td>0.916907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.248560</td>\n",
       "      <td>0.268356</td>\n",
       "      <td>0.912346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.087379</td>\n",
       "      <td>0.035617</td>\n",
       "      <td>0.992368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.073909</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>0.996205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.448867</td>\n",
       "      <td>0.496245</td>\n",
       "      <td>0.365006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.131332</td>\n",
       "      <td>0.053066</td>\n",
       "      <td>0.991671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.417008</td>\n",
       "      <td>0.082446</td>\n",
       "      <td>0.856370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.298470</td>\n",
       "      <td>0.048697</td>\n",
       "      <td>0.981877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.399138</td>\n",
       "      <td>0.323082</td>\n",
       "      <td>0.966786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.494449</td>\n",
       "      <td>0.180471</td>\n",
       "      <td>0.851839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.634293</td>\n",
       "      <td>0.063633</td>\n",
       "      <td>0.929706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.024915</td>\n",
       "      <td>0.029721</td>\n",
       "      <td>0.999026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.082329</td>\n",
       "      <td>0.218497</td>\n",
       "      <td>0.974563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.361916</td>\n",
       "      <td>0.060476</td>\n",
       "      <td>0.973168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.429078</td>\n",
       "      <td>0.090160</td>\n",
       "      <td>0.932082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.197482</td>\n",
       "      <td>0.086375</td>\n",
       "      <td>0.996517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.034745</td>\n",
       "      <td>0.036814</td>\n",
       "      <td>0.993598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.061985</td>\n",
       "      <td>0.071989</td>\n",
       "      <td>0.995603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.190196</td>\n",
       "      <td>0.065055</td>\n",
       "      <td>0.939905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.153260</td>\n",
       "      <td>0.016271</td>\n",
       "      <td>0.991497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.233868</td>\n",
       "      <td>0.059293</td>\n",
       "      <td>0.993946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.104784</td>\n",
       "      <td>0.047518</td>\n",
       "      <td>0.983167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.377644</td>\n",
       "      <td>0.037105</td>\n",
       "      <td>0.946890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.081579</td>\n",
       "      <td>0.141124</td>\n",
       "      <td>0.967623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.129755</td>\n",
       "      <td>0.214087</td>\n",
       "      <td>0.954455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.174862</td>\n",
       "      <td>0.133242</td>\n",
       "      <td>0.982040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.107839</td>\n",
       "      <td>0.396587</td>\n",
       "      <td>0.878742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.393867</td>\n",
       "      <td>0.285136</td>\n",
       "      <td>0.844094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2\n",
       "60  0.089724  0.006591  0.992500\n",
       "61  0.093017  0.103669  0.959554\n",
       "62  0.108823  0.043440  0.996305\n",
       "63  0.292536  0.638150  0.916907\n",
       "64  0.248560  0.268356  0.912346\n",
       "65  0.087379  0.035617  0.992368\n",
       "66  0.073909  0.009973  0.996205\n",
       "67  0.448867  0.496245  0.365006\n",
       "68  0.131332  0.053066  0.991671\n",
       "69  0.417008  0.082446  0.856370\n",
       "70  0.298470  0.048697  0.981877\n",
       "71  0.399138  0.323082  0.966786\n",
       "72  0.494449  0.180471  0.851839\n",
       "73  0.634293  0.063633  0.929706\n",
       "74  0.024915  0.029721  0.999026\n",
       "75  0.082329  0.218497  0.974563\n",
       "76  0.361916  0.060476  0.973168\n",
       "77  0.429078  0.090160  0.932082\n",
       "78  0.197482  0.086375  0.996517\n",
       "79  0.034745  0.036814  0.993598\n",
       "80  0.061985  0.071989  0.995603\n",
       "81  0.190196  0.065055  0.939905\n",
       "82  0.153260  0.016271  0.991497\n",
       "83  0.233868  0.059293  0.993946\n",
       "84  0.104784  0.047518  0.983167\n",
       "85  0.377644  0.037105  0.946890\n",
       "86  0.081579  0.141124  0.967623\n",
       "87  0.129755  0.214087  0.954455\n",
       "88  0.174862  0.133242  0.982040\n",
       "89  0.107839  0.396587  0.878742\n",
       "90  0.393867  0.285136  0.844094"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result.iloc[60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'ROC Curves'}, xlabel='False Positive Rate', ylabel='True Positive Rate'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABXc0lEQVR4nO3dd3gUVffA8e9JQu+EIr0G00MLTRGQqiBFEORVmgUBhVcsoPgTEQtSRKnSVFRQUHwF7AqCIL0ISAcpIRAgEAglJCTZ8/tjN0tC2gLZbDa5n+fZh8zu3ZkzQ7JnZ+6dc0VVMQzDMIz0eLg6AMMwDCNnM4nCMAzDyJBJFIZhGEaGTKIwDMMwMmQShWEYhpEhkygMwzCMDJlEYRiGYWTIJArD7YnIMRG5JiJXROS0iMwXkaI3tWkmIn+IyGURiRaR70XE/6Y2xUXkQxEJs63rsG25TDrbFREZJiK7ReSqiISLyDciEuTM/TWM7GYShZFbPKSqRYG6QD3g1aQXRKQp8BuwDKgI1AB2AutEpKatTX5gJRAAdACKA82A80CjdLY5BfgvMAwoDdQBlgIdbzV4EfG61fcYRnYxicLIVVT1NPAr1oSRZALwuapOUdXLqhqlqv8HbATG2Nr0BaoC3VR1r6paVPWsqr6lqj/dvB0R8QGeBXqr6h+qGqeqMaq6UFXfs7VZLSJPJXtPfxH5K9myisizInIIOCQis0Rk0k3bWSYiL9h+rigi34pIpIgcFZFhydo1EpGtInJJRM6IyOTbP4qGkZJJFEauIiKVgQeAw7blwljPDL5Jo/nXQFvbz22AX1T1ioObag2Eq+rmO4uYrkBjwB/4EuglIgIgIqWAdsAiEfEAvsd6JlTJtv3nRaS9bT1TgCmqWhyoZds3w8gSJlEYucVSEbkMnADOAm/Yni+N9fc8Io33RABJ/Q/e6bRJz622T8842xnONWAtoEBz22s9gA2qegoIBcqq6lhVva6qR4C5wKO2tvFAbREpo6pXVHVjFsRmGIBJFEbu0VVViwEtAV9uJIALgAWokMZ7KgDnbD+fT6dNem61fXpOJP2g1gqdi4Detqf+Ayy0/VwNqCgiF5MewCigvO31J7H2kewXkS0i0ikLYjMMwCQKI5dR1T+B+cAk2/JVYAPwSBrNe2LtwAZYAbQXkSIObmolUFlEGmbQ5ipQONnyXWmFfNPyV0APEamG9ZLUt7bnTwBHVbVkskcxVX0QQFUPqWpvoBwwHlhyC/tiGBkyicLIjT4E2opIXdvyK0A/21DWYiJSSkTeBpoCb9rafIH1w/hbEfEVEQ8R8RaRUSLy4M0bUNVDwEzgKxFpKSL5RaSgiDwqIq/Ymu0AHhaRwiJSG+u3/gyp6t9AJDAP+FVVL9pe2gxcEpGRIlJIRDxFJFBEQgFE5HERKauqFiDpPYkOHi/DyJBJFEauo6qRwOfA67blv4D2wMNY+xWOYx1Ce6/tAx9VjcPaob0f+B24hPXDuQywKZ1NDQOmAzOwfjj/C3TD2ukM8AFwHTgDfMaNy0iZ+coWy5fJ9ikReAjraK6jWC+ZzQNK2Jp0APaIyBWsHduPqmqsg9szjAyJmbjIMAzDyIg5ozAMwzAyZBKFYRiGkSGTKAzDMIwMmURhGIZhZMjtCpGVKVNGq1ev7uowDMMw3Mq2bdvOqWrZ23mv2yWK6tWrs3XrVleHYRiG4VZE5PjtvtdcejIMwzAyZBKFYRiGkSGTKAzDMIwMmURhGIZhZMgkCsMwDCNDJlEYhmEYGXLa8FgR+QToBJxV1cA0XhesVS4fBGKA/qq63Vnx5CTnzsWwdespkgoyFimSn/vuq5Zm2337Ijl27KJ92de3DDVqlEqz7a+/HsZiuVHksV27Wnh6pv4ucPLkJXbtOmNfrlixGCEhaU2VABs3hnPhwjX7cuPGlSldulCqdleuXGft2huj78w+mX0y+5Tz9ul2OfM+ivlYSzB/ns7rDwA+tkdj4CPbv27rs892cOTIBc6di+HcuWtMmdKBu+4qmqrdzp2neeCBGxWnfX3LsG/fsykblZ0OwJwrV/kw9ka16MmT2zF8eNMb7T7fDS+uBqDbufNcS7aKq1dHUbhwsl/s1othVySrYuPoc+XG1NCPPRbEggUPJwvwLLSxTrn88sVo/kpIsL+0Zk1/mjdP9gv7wh/wxV5OJCTw4MVos09mn8w+5cB9uv7CCu6E0xKFqq4RkeoZNOkCfG6b/nGjiJQUkQqqmuE8xNuiLiEL72ynb8m+aFhzFi7HWx/1S0OXKmm3Hf03hMXYF78O8oIaqRPFCO/3uDEtMsTqPrZdlxRtGjAtzU2cSHiBbddvTIdcJqEZ1ewzZ6b09/UiFPSKty/76ssUoWqqdlGWhWy73t2+XDi+Cn6MSHOdB+Lvo/D1MPtyVcujlOWeVO3MPpl9MvuUM/Zp+c/V+d/FuDTbOsqVd2ZXItl8wUC47blUiUJEBgIDAajhc+dbvp4IS8IgMg7OxsLVBPgw7Rktx3wRzpiwi/blUkXzcyGNdvVPxFIqIt4+ryZgTSxpeHxJBSYkW/a8lPrU0zAMIyv4VSnC3sTbvikbcG2ikDSeS3MWJVWdA8wBkJp1VB9rk+YKL1y4xsGD5zly5AJHjlygZcvq3HNP6oxvsSiFB79DXFyymSIffwiI5eaJnH6acBySJYrQwkX4Na3t7zzLo2/8k+KpL4Lu5vHHglM1Dd/wCW3z5cPTtlylTiAN8t+869bTSl9PTzrkywetraenzWt/QoP8fjeaee0GVgPQLn8+4ioWBV9vABoUjKFg/mT/xbIYiKSih4d1nY3ugmIFaFm3DQ3yf3ujXb6zgPVUuXE+L4oWLwCNKgDQpNwWAvOXu9HW4w9gL0VErOv0LQ2VilGlSn0a5J9u9snsk9mnbN4nv5LvsPnjAwwePNgad+BuGpdaQs0Lb3K7nDrDne3S0w/pdGbPBlar6le25QNAy8wuPUnNOqpHDqb52iuvrGD8+HX25dGFCvFmkcIQ+Vyqtn5+M9i//5x9+bVCR6nqGcfAy1+laLc5dD6Nt97IxvV8y7D95muFADvPsqj5F+xJSKSsh1CmWgnu+akn1aqVTNU08vnelF2Y7PTy/ZbQN9UhMgzDcFhCQgJTp05l9OjRXL16lTVr1tC8+Y1L3CKyTVXTvnSSCVeeUSwHnhORRVg7saMzSxKZqVkzZU//EUv6c8vXrFkqRaKo6xVEjwIFUrXzKZSfuUWLUFY8KOMhVJjUPt11PlqgACSt4q4SkEaSMAzDyGqbNm3imWeeYefOnQB0796dmjVrZtn6nTk89iugJVBGRMKBN4B8AKo6C/gJ69DYw1iHxw64022mShSJlnTbPvdcKL0rl6Tm4gPU9PSkvAhHEs9x86EtteY/POXIxkPKpXnmkpawCYsIm7AojctNhmEYjrtw4QKjRo1i9uzZqCrVq1dn+vTpdOzYMUu348xRT2kPHbjxugJpXMPJmF+pQ6l6/5Ncq1qS2oG9CThYmZoengR7WXsB0mpfrjX4RzSj2v+sYcZVPUfC+EVsu3771/EMwzCy05tvvsmsWbPw8vLipZde4vXXX6dw4cJZvh2n9lE4g1990RcnVaXevWGZN87BisuD+OT70dVhGIbhZhISEvDysn7HP3fuHE8++STvvPMOgYEZ93PeSR+F2yWKkmVK6qWo4Uya1I7hw5sgX+xh4MBnAJgzZ7bpFDYMI1eKjY1l/PjxLF26lE2bNpE/f/5ben+eShQiFRWsieGJJ+ry0bIw8kuyS0sO9hMYhmG4i5UrVzJ48GAOHToEwPLly3nooYduaR13kijcuijgn38e54qbJTrDMAxHnTlzhscff5w2bdpw6NAh/Pz8WL169S0niTvldonCw8OaGEqUKMAPP/yH0iMbMyd2XSbvMgzDcC8LFizA19eXhQsXUrBgQd5991127NhBixYtsj0WV95HcVuq+57DK8GbadMewNe3DPiWgbdsL1Yr7tLYDMMwsorFYuHixYt06NCBGTNmZOl9EbfK7RJFiRPebGpQhnztaqV4/kjiOWpO6uyiqAzDMO7MlStX2LBhA23btgWgT58+VKxYkdatWyOS9i0B2cXtOrMb5quqW0uOSNFpnXQQ3W1fDMMwAJYuXcrQoUOJjIxk9+7d1K5dO8u3kWc7sw3DMNzZ8ePH6dKlC926dSM8PJygoCDi4u6sJLgzmERhGIaRzeLj45k4cSL+/v4sX76cYsWKMW3aNDZu3EhAQICrw0vF7fooYn1Ow1c9XR2GYRjGbRs2bBizZs0CoGfPnnzwwQdUrFjRxVGlz+3OKCyF4q0F+AzDMNzU888/j5+fHz///DOLFy/O0UkC3DBRGIZhuBNV5YsvvqB37972ATd33303u3fvpkOHDi6OzjEmURiGYTjJgQMHaN26NX379mXRokX8/PPP9tc8PNzn49d9IjUMw3AT165dY/To0QQHB7Nq1Sq8vb2ZP38+DzzwgKtDuy1u15ltGIaRk61YsYJBgwbx77//AvDkk08yfvx4vL29XRzZ7XO7MwqPa/lg51lXh2EYhpGm9evX8++//xIQEMDatWuZN2+eWycJcMMzioKH7oI2X5ty4oZh5AiJiYkcPnyYu+++G4CRI0dSpkwZnnrqqVueMyKncrszCsMwjJzi77//plmzZtx7771ERUUBUKBAAYYMGZJrkgS4caIQEfvDMAwjO12+fJnhw4fTsGFDNm/eTIECBex9ErmR2yUKS6HrbEtIPV/2gw8+6IJoDMPIS1SVb7/9Fj8/Pz788EMAhg8fzr59+wgNDXVtcE7kdtVj/RuI7t3mXjEbhpE7/Pe//2Xq1KkAhIaGMnv2bOrVq+fiqBxjqscahmFkg27dulGiRAlmzJjBhg0b3CZJ3Cm3G/VkGIaRXf766y9WrVrF66+/DkDLli0JCwujePG8NZumSRSGYRg3OX/+PCNHjuTjjz8GoHXr1jRr1gwgzyUJMInCMAzDTlX5/PPPeemllzh37hz58uXjlVdeyTOXmNJjEoVhGAawb98+Bg8ezJ9//glAq1atmDlzJr6+vi6OzPXcrjM7f3gpeOEPV4dhGEYuM3nyZP7880/Kli3LF198wcqVK02SsHG7MwqvqKLwxV6YfL+rQzEMw81FR0dTokQJAMaNG0eRIkUYPXo0pUuXdnFkOYvbnVEYhmHcqVOnTtGrVy+aNGnC9evXAShTpgwffvihSRJpMInCMIw8IzExkWnTpuHr68vXX39NWFgY27dvd3VYOZ7bJYrrlaLg/ZauDsMwDDezbds2GjduzLBhw7h8+TKdO3dm3759NGnSxNWh5XhOTRQi0kFEDojIYRF5JY3XS4jI9yKyU0T2iMiAzNaZ4H0V+gY6J2DDMHKlMWPG0KhRI7Zt20aVKlVYunQpy5Yto2rVqq4OzS04LVGIiCcwA3gA8Ad6i4j/Tc2eBfaqagjQEnhfRHJPbV7DMHKEmjVrIiK8+OKL7N27ly5durg6JLfizFFPjYDDqnoEQEQWAV2AvcnaKFBMrLXCiwJRQIITYzIMIw84cuQIW7ZsoVevXgD06dOHxo0b2ycXMm6NMy89VQJOJFsOtz2X3HTADzgF/AP8V1UtN69IRAaKyFYR2eqsYA3DcH/Xr1/n3XffJSAggH79+nH48GHAOn+NSRK3z5mJIq0ZhW6uD94e2AFUBOoC00UkVSEVVZ2jqg1vt0SuYRi535o1a6hbty6vvfYasbGx9OjRI0/WZXIGZyaKcKBKsuXKWM8ckhsA/E+tDgNHAXMrpGEYDjt37hwDBgygRYsW7Nu3Dx8fH1asWMGCBQsoV66cq8PLFZyZKLYAPiJSw9ZB/Siw/KY2YUBrABEpD9wNHMlopYV3VYGy050QrmEY7mjQoEHMnz+fAgUK8Oabb7Jr1y5at27t6rByFad1Zqtqgog8B/wKeAKfqOoeERlke30W8BYwX0T+wXqpaqSqnnNWTIZh5A4WiwUPD+v33HfeeYdr167x4Ycf4uPj4+LIcie3mwq1Yb6qurXkCIh8ztWhGIaRzWJiYnjrrbfYsWMHP/30E9YBk4Yj7mQqVLcrCmgYRt70448/8txzz3Hs2DFEhM2bN9O4cWNXh5UnuF0Jj5jgE+ZswjDykPDwcLp3706nTp04duwYISEhrF+/3iSJbOR2icIwjLxj5syZ+Pn58b///Y8iRYowefJktm7dauozZTNz6ckwjBzr3LlzXLlyhW7dujFlyhSqVKmS+ZuMLOd2ndn+DUT3bnOvmA3DcMzFixfZv3+//YwhLi6OVatW0aFDBxdH5v7upDPbXHoyDMPlVJVFixbh5+dH586diYqKAqBAgQImSeQAJlEYhuFShw8fpkOHDvTu3ZvTp0/j4+NDdHS0q8MyknG7ROF1vgh8vtvVYRiGcYfi4uJ46623CAwM5LfffqNUqVLMnTuXtWvXUqNGDVeHZyTjcGe2iBRR1avODMYR+U+WhhdXm8mLDMPN9erVi2XLlgHQt29fJk6caGoz5VCZnlGISDMR2Qvssy2HiMhMp0dmGEau9vzzz+Pr68sff/zBZ599ZpJEDubIGcUHWMuBLwdQ1Z0icp9TozIMI1exWCx88skn7Nu3j/fffx+Ali1bsnv3bjw9PV0cnZEZhy49qeqJm2qqJDonnMwllL4Cj908o6phGDnVP//8w6BBg1i/fj1gvcwUEhICYJKEm3CkM/uEiDQDVETyi8hL2C5DucL1yhdg8v2u2rxhGA66evUqI0aMoF69eqxfv5677rqLRYsWERwc7OrQjFvkyBnFIGAK1mlMw4HfgCHODMowDPf2/fff89xzzxEWFoaI8Oyzz/LOO+9QokQJV4dm3AZHEsXdqvpY8idE5B5gnXNCMgzD3S1dupSwsDDq1avH7NmzCQ0NdXVIxh1w5NLTNAefMwwjj0pISOD48eP25fHjxzNt2jQ2b95skkQukO4ZhYg0BZoBZUXkhWQvFcc6Y51hGAYbN25k0KBBxMXFsXPnTvLnz0+ZMmV47jkzHUBukdEZRX6gKNZkUizZ4xLQw/mhGYaRk124cIHBgwfTrFkzdu7cSWxsLMeOHXN1WIYTZFo9VkSqqerxDBtlo/rF79LtoVNgZS9Xh2IYeZKq8tVXXzF8+HDOnj2Ll5cXL7/8Mv/3f/9H4cKFXR2ekQ5nT4UaIyITgQCgYNKTquqSMaoe1/LDrkhXbNowDOCxxx7jq6++AqB58+Z89NFHBAQEuDgqw5kc6cxeCOwHagBvAseALU6MyTCMHKxDhw54e3vzySefsHr1apMk8gBHEoW3qn4MxKvqn6r6BGDmITSMPGLFihXMnj3bvtynTx8OHjzIgAED8PBwuwLUxm1w5H853vZvhIh0FJF6QGUnxpShWJ/TsKKnqzZvGHnGmTNneOyxx2jbti3//e9/+ffffwEQEUqXLu3i6Izs5EgfxdsiUgJ4Eev9E8WB550ZVEYsheIhxFSZNAxnsVgszJkzh1deeYXo6GgKFizI6NGjzXzVeVimiUJVf7D9GA20Avud2YZh5DI7d+7kmWeeYdOmTQA88MADTJ8+nZo1a7o4MsOVMrrhzhPoibXG0y+qultEOgGjgEJAvewJ0TCM7DJixAg2bdpExYoVmTJlCt27d+emytFGHpTRGcXHQBVgMzBVRI4DTYFXVHVpNsRmGIaTqSoxMTEUKVIEgKlTpzJr1izefPNNihcv7uLojJwi3RvuRGQ3EKyqFhEpCJwDaqvq6ewM8Gb+DUT3bsv4JkHDMDJ3/Phxhg4dytWrV1mxYoU5c8jl7uSGu4xGPV1XVQuAqsYCB12dJAA8ruWDnWddHYZhuK34+HgmTJiAv78/33//PVu2bOHQoUOuDsvIwTJKFL4issv2+CfZ8j8isiu7ArxZwUN3QZuvXbV5w3Br69ato379+owcOZKYmBh69erF/v37qVOnjqtDM3KwjPoo/LItCsMwnG7o0KFMnz4dgJo1azJjxgw6dOjg4qgMd5BuoshJhQANw7hzZcuWJV++fIwcOZJRo0ZRqFAhV4dkuAmn3n8vIh1E5ICIHBaRV9Jp01JEdojIHhH5M7N1Wgpdh+CyWR+sYeQy+/fv57fffrMvjxw5kl27dvHWW2+ZJGHckkzLjN/2iq33YRwE2mKda3sL0FtV9yZrUxJYD3RQ1TARKaeqGfZUm1FPhpGxa9eu8e677zJ+/HhKlizJ/v37TckNw+llxhGRQkBVVT1wC+tuBBxW1SO2dSwCugB7k7X5D/A/VQ0DyCxJGIaRsd9++40hQ4bY6zJ17tzZDHs17liml55E5CFgB/CLbbmuiCx3YN2VgBPJlsNtzyVXByglIqtFZJuI9HUoasMwUoiIiODRRx+lffv2/PvvvwQEBLB27VrmzZtHqVKlXB2e4eYc6aMYg/Xs4CKAqu4AqjvwvrS+xtx8zcgLaAB0BNoDr4tIqnF6IjJQRLaKyFYHtmsYec7DDz/M4sWLKVSoEOPHj+fvv//m3nvvdXVYRi7hSKJIUNXo21h3ONYSIEkqA6fSaPOLql5V1XPAGiDk5hWp6hxVbXi719cMIzdK3r/43nvv0alTJ/bu3cuIESPIly+fCyMzchtHEsVuEfkP4CkiPiIyDWsHdGa2AD4iUkNE8gOPAjdfsloGNBcRLxEpDDQG9t1C/IaR51y+fJnhw4fzzDPP2J9r0aIF33//PdWrV3ddYEau5UiiGIp1vuw44Eus5cafz+xNqpoAPAf8ivXD/2tV3SMig0RkkK3NPqx9H7uwFh+cp6q7M1pv/vBS8MIfDoRtGLmLqvLtt9/i5+fHhx9+yKeffsqxY8dcHZaRB2Q6PFZE6qnq39kUT6Ya5quqW0uOgMjnXB2KYWSbo0eP8txzz/HTTz8B0KhRI2bNmkW9eqbav+EYZxUFTDJZRPaLyFsiYmZRN4xspKqMHz+egIAAfvrpJ0qUKMHMmTNZv369SRJGtsk0UahqK6AlEAnMsRUF/D9nB2YYhnV+6oMHD3Lt2jV69+7N/v37GTx4MJ6enq4OzchDbunObBEJAkYAvVQ1v9OiykBw9aK6a+xG6Bvois0bhtOdO3eO06dPExgYaF/++++/adu2rYsjM9yZUy89iYifiIyxTWQ0HeuIp8q3s7GskOB91SQJI1dSVebPn4+vry+PPPII169fB6BMmTImSRgu5UgJj0+Br4B2qnrzfRCGYWSBffv2MWjQINasWQNASEgIFy5coHz58i6OzDAcSBSq2iQ7AjGMvCgmJoZ33nmHiRMnEh8fT9myZZk8eTKPPfaYqdFk5BjpJgoR+VpVe9pmt0vekSGAqmqw06MzjFxMVbn//vvZtGkTAM888wzjxo0ztZmMHCejM4r/2v7tlB2BGEZeIyIMGTKEmJgYZs+eTdOmTV0dkmGkyZEb7sar6sjMnssuZj4Kw10lJiYyc+ZM4uPjeeGFFwDrWUVCQoKpzWQ4nbNvuEtruMUDt7OxrFB4VxUoO91VmzeM27J161YaN27MsGHDGDVqFKdOWceFiIhJEkaOl26iEJHBtv6Ju0VkV7LHUay1mQzDyER0dDRDhw6lUaNGbNu2jSpVqrB48WIqVqzo6tAMw2EZ9VF8CfwMjAOSz3d9WVWjnBqVYbg5VeWbb77h+eefJyIiAk9PT4YPH84bb7xB0aJFXR2eYdySjBKFquoxEXn25hdEpLRJFoaRsdmzZxMREUGTJk2YNWsWISGpploxDLeQbme2iPygqp1sl5qUlDPWqarWzI4Ab2Y6s42cKi4ujosXL9pvkjtw4ACrV6/m6aefxsPDke5Aw3CeO+nMvqVaTzmBSRRGTvTnn38yaNAgKlasyIoVK8zNckaO4+xaT/eISBHbz4+LyGQRqXo7GzOM3CYyMpL+/fvTsmVL9u/fz4kTJzhz5oyrwzKMLOXI+fBHQIyIhGCtHHsc+MKpURlGDmexWPj444/x9fXls88+o0CBArz55pvs2rWLu+66y9XhGUaWcqQoYIKqqoh0Aaao6sci0s/ZgRlGTqWqtG/fnhUrVgDQpk0bZs6ciY+Pj4sjMwzncOSM4rKIvAr0AX4UEU/A3CFk5FkiQvPmzSlfvjxffvklv/32m0kSRq7mSAmPu4D/AFtUda2tf6Klqn6eHQHezExcZLjCjz/+SHx8PF27dgWsI5yuXbtGyZIlXRqXYTjKqZ3ZqnoaWAiUEJFOQKyrkgRA/pOl4cXVrtq8kceEh4fTvXt3OnXqxNNPP01UlPX2oQIFCpgkYeQZjox66glsBh4BegKbRKSHswMzDFdKSEjggw8+wM/Pj//9738UKVKEUaNGUbx4cVeHZhjZzpHO7NeAUFU9CyAiZYEVwBJnBmYYrrJ582aeeeYZduzYAUC3bt2YMmUKVapUcW1ghuEijiQKj6QkYXMexzrBnSKh9BV4zN9VmzdyOYvFwoABA9i7dy9Vq1Zl+vTpPPTQQ64OyzBcypFE8YuI/Ip13myAXsBPzgspY9crX4DJ97tq80YupKrExcVRsGBBPDw8mDFjBj///DOjR4+mSJEirg7PMFzOoRIeIvIwcC/Wek9rVPU7ZweWHlPCw8hKhw8fZsiQIVSpUoWPP/7Y1eEYhtM4ZdSTiPiIyDIR2Y21I/t9VR3uyiRhGFklLi6OsWPHEhgYyO+//87SpUs5f/68q8MyjBwpo76GT4AfgO7ANmBatkRkGE72xx9/EBwczBtvvEFcXBz9+vVj//79eHt7uzo0w8iRMuqjKKaqc20/HxCR7dkRkGE4S2JiIgMGDOCLL6ylyu6++25mzZpFy5YtXRuYYeRwGSWKgiJSjxvzUBRKvqyqJnEYbsXT0xMvLy8KFizI//3f//HSSy9RoEABV4dlGDleRhMXrcrgfaqqLhl6VL/4Xbo9dAqs7OWKzRtu5p9//iE2NpbQ0FAAzp8/z8WLF6lVq5aLIzOM7HUnndnpnlGoaqvbD8l5PK7lh12Rrg7DyOGuXr3KmDFj+OCDD/Dx8WHnzp3kz58fb29v0xdhGLfIkfsoDMOtLF++nKFDhxIWFoaI0KZNG+Lj48mfP7+rQzMMt+TUO6xFpIOIHBCRwyLySgbtQkUk0dSQMu5EWFgYXbt2pUuXLoSFhVG/fn02b97MtGnTzI1zhnEHnHZGYZu3YgbQFggHtojIclXdm0a78cCvjqw31uc0fNUzq8M13FxiYiItW7bk6NGjFCtWjLfffpshQ4bg5WVOmg3jTjlSPVZsc2WPti1XFZFGDqy7EXBYVY+o6nVgEdAljXZDgW+Bs2m8loqlUDyElHOkqZEHJA3G8PT0ZMyYMfTo0YN9+/YxbNgwkyQMI4s48pc0E7AA9wNjgctYP9hDM3lfJeBEsuVwoHHyBiJSCehmW3e66xORgcBAAL/6DkRsZJn4+HjCw8OJjY11dSgpWCwWLly4gJeXFyVKlAAgNDSU0NBQLl26xKVLl1wcoWG4RsGCBalcuTL58mXdRKSOJIrGqlpfRP4GUNULIuJIr6Ck8dzNY3E/BEaqaqJIWs1tb1KdA8wBa60nB7ZtZJHw8HCKFStG9erVyej/KLuoKlFRUZw4ccJexM/Hx8ecPRgG1r+P8+fPEx4eTo0aNbJsvY78dcXb+hEU7PNRWBx4XziQvIB/ZeDUTW0aAotsH0BlgAdFJEFVlzqwfiMbxMbG5pgkERsby/Hjx7l8+TIARYsWpVq1aiZJGIaNiODt7U1kZNbeQuDIX9hU4DugnIi8A/QA/s+B920BfESkBnASeBTr3Nt2qmpPeSIyH/jBJImcx9VJQlWJiIggIiICVcXLy4vKlSvj7e3t8tgMI6dxxt9EpolCVReKyDagNdbLSV1VdZ8D70sQkeewjmbyBD5R1T0iMsj2+qzbCdjjWj7YedZ0aOcxly9fRlUpU6YMlSpVytLrr4ZhZMyRUU9VgRjge2A5cNX2XKZU9SdVraOqtVT1Hdtzs9JKEqraX1UznV614KG7oM3XjmzecGPx8fHExcUB4OXlRc+ePenbty9Dhw7l6tWr9nZ79uzh/vvvp06dOvj4+PDWW2/ZR0IB/PzzzzRs2BA/Pz98fX156aWXsn1fblfv3r0JDg7mgw8+cKh90aJFnRKHqjJs2DBq165NcHAw27enXeZNVbn//vtz9ECCDh06ULJkSTp16pRum7i4OHr16kXt2rVp3Lgxx44ds7/22Wef4ePjg4+PD5999pn9+UcffZRDhw45M3TXUtUMH8A/wC7bv4eABGBPZu9z1qOBVxXVMtPUyB579+7N1u1ZLBY9c+aMbt++Xffv368Wi0WLFClif71v37769ttvq6pqTEyM1qxZU3/99VdVVb169ap26NBBp0+frqqq//zzj9asWVP37dunqqrx8fE6Y8aMLI03Pj4+S9eXJCIiQqtWrXpL70l+nLLSjz/+qB06dFCLxaIbNmzQRo0apdnuhx9+0Oeff/6W1p2QkJAVITpsxYoVunz5cu3YsWO6bWbMmKHPPPOMqqp+9dVX2rNnT1VVPX/+vNaoUUPPnz+vUVFRWqNGDY2KilJV1dWrV+tTTz3l/B1wUFp/t8BWvc3P3UzPKFQ1SFWDbf/6YL0/4i/npS4jpxIRpzySxMTEsH//fsLCwkhMTMTDwwOLJeW4iaZNm3Ly5EkAvvzyS+655x7atWsHQOHChZk+fTrvvfceABMmTOC1117D19cXsJ6ZDBkyJNV+XblyhQEDBhAUFERwcDDffvstkPIb+pIlS+jfvz8A/fv354UXXqBVq1a8/PLLVK9enYsXL9rb1q5dmzNnzhAZGUn37t3tw3bXrVuXatuxsbH2bderV49Vq6y1ONu1a8fZs2epW7cua9euTfGeM2fO0K1bN0JCQggJCWH9+vWp9qd169bUr1+foKAgli1bBljrX3Xs2JGQkBACAwNZvHgxAK+88gr+/v4EBweneca1bNky+vbti4jQpEkTLl68SERERKp2CxcupEuXG7dKde3alQYNGhAQEMCcOXPszxctWpTRo0fTuHFjNmzYwIIFC2jUqBF169blmWeeITExEYDBgwfTsGFDAgICeOONN1Jt73a0bt2aYsWKZdhm2bJl9OvXD4AePXqwcuVKVJVff/2Vtm3bUrp0aUqVKkXbtm355ZdfAGjevDkrVqwgISEhS+LMaW55uIiqbheRzO6hcBpLoesQXNZVmzecIDExkVOnTnHmzBkA8uXLR5UqVShVqlSKRJKYmMjKlSt58sknAetlpwYNGqRYV61atbhy5QqXLl1i9+7dvPjii5lu/6233qJEiRL8888/AFy4cCHT9xw8eJAVK1bg6emJxWLhu+++Y8CAAWzatInq1atTvnx5/vOf/zB8+HDuvfdewsLCaN++Pfv2pezemzFjBmCtcrt//37atWvHwYMHWb58OZ06dWLHjh2ptj1s2DBatGjBd999R2JiIleuXEnxesGCBfnuu+8oXrw4586do0mTJnTu3JlffvmFihUr8uOPPwIQHR1NVFQU3333Hfv370dEUiS8JCdPnqRKlRsDGCtXrszJkyepUKFCinbr1q1j9uzZ9uVPPvmE0qVLc+3aNUJDQ+nevTve3t5cvXqVwMBAxo4dy759+xg/fjzr1q0jX758DBkyhIULF9K3b1/eeecdSpcuTWJiIq1bt2bXrl0EBwen2ObEiRNZuHBhqpjvu+8+pk6dmsb/XOaS72/SfTrnz59P9zgAeHh4ULt2bXbu3JnqdzI3yDRRiMgLyRY9gPqAy8q3xvqcMSXGXUQdmF/9VlksFvbs2WPvjyhXrhyVKlXC09PT3ubatWvUrVuXY8eO0aBBA9q2bWuPJ70RHrcy8mPFihUsWrTIvlyqVKlM3/PII4/YY+zVqxdjx45lwIABLFq0iF69etnXu3fvjYo1ly5d4vLlyym+0f71118MHToUAF9fX6pVq8bBgwcpXrx4utv+448/+PzzzwHrHelJNxwmUVVGjRrFmjVr8PDw4OTJk5w5c4agoCBeeuklRo4cSadOnWjevDkJCQkULFiQp556io4dO6Z57T6t//e0jm9UVFSKfZs6dSrffWedOfnEiRMcOnQIb29vPD096d69OwArV65k27Zt9jLw165do1w560CVr7/+mjlz5pCQkEBERAR79+5NlShefvllXn755XSP1e1Ib38zOw7lypXj1KlTuTJROFIUsFiyRwHgR9IuxWEYt8zDwwNvb28KFy6Mn58fVatWTZEkAAoVKsSOHTs4fvw4169ft38LDwgIYOvWrSnaHjlyhKJFi1KsWDECAgLYtm1bpjGkl3CSP3fznenJiww2bdqUw4cPExkZydKlS3n44YcBaxLcsGEDO3bsYMeOHZw8eTLVZQ9nJN+FCxcSGRnJtm3b2LFjB+XLlyc2NpY6deqwbds2goKCePXVVxk7dixeXl5s3ryZ7t27s3TpUjp06JBqfZUrV+bEiRtFFsLDw6lYsWKqdl5eXvZLhatXr2bFihVs2LCBnTt3Uq9ePfsxLFiwoP3/WFXp16+f/RgdOHCAMWPGcPToUSZNmsTKlSvZtWsXHTt2TLM6wMSJE6lbt26qx7Bhw277+CXf34SEBKKjoyldunSmxyE2NpZChQrd9nZzsgwThe1Gu6Kq+qbt8Y6qLlTVnFXPwXAbFouF06dPExUVZX/urrvuws/PL9MKryVKlGDq1KlMmjSJ+Ph4HnvsMf766y9WrFgBWL+NDhs2jBEjRgDWb5vvvvsuBw8etG978uTJqdbbrl07pk+fbl9OuvRUvnx59u3bZ7+0lB4RoVu3brzwwgv4+fnZ57u4eb1pXUa677777JdODh48SFhYGHfffXeGx6F169Z89NFHgPVy3M2jjKKjoylXrhz58uVj1apVHD9+HIBTp05RuHBhHn/8cV566SW2b9/OlStXiI6O5sEHH+TDDz9MM8bOnTvz+eefo6ps3LiREiVKpLrsBNapZY8cOWKPoVSpUhQuXJj9+/ezcePGdPdlyZIlnD1rLfUWFRXF8ePHuXTpEkWKFKFEiRKcOXOGn3/+Oc33v/zyy/Ykk/xxu5edkvY3aUTTkiVLuP/++xER2rdvz2+//caFCxe4cOECv/32G+3bt7e/7+DBgwQEBNz2dnO09Hq5AS/bvytvt6fcGQ+/+jjW7W9kiawc9XT58mXdvXu3btmyRf/++2+HR7zcPJqnU6dO+vnnn6uq6q5du7RFixZap04drVWrlo4ZM0YtFou97ffff6/169dXX19f9fPz05deeinNuPr27asBAQEaHBys3377raqqfvPNN1qzZk1t0aKFPvvss9qvXz9VVe3Xr59+8803KdaxZcsWBXT+/Pn25yIjI7Vnz54aFBSkfn5+9pE0yV27dk379eungYGBWrduXf3jjz9UVfXo0aMaEBCQ5vE4ffq0du7cWQMDAzUkJETXr1+f4jhFRkZqkyZNtEGDBvrkk0+qr6+vHj16VH/55RcNCgrSkJAQbdiwoW7ZskVPnTqloaGhGhQUpIGBgSniT2KxWHTIkCFas2ZNDQwM1C1btqQZ19ixY3Xu3LmqqhobG6sdOnTQoKAg7dGjh7Zo0UJXrVqVIs4kixYt0pCQEA0KCtL69evrhg0b7MfZ19dXH3zwQe3WrZt++umnaW73Vtx7771apkwZLViwoFaqVEl/+eUXVVV9/fXXddmyZapq/T/p0aOH1qpVS0NDQ/Xff/+1v//jjz/WWrVqaa1atfSTTz6xP3/69GkNDQ294/iySlaPespoKtTtaq3x9D7gA3wD2Aewq+r/nJ3E0uLfQHTvNlPuKbvs27cPPz+/O1pHQkIC4eHhnDt3DoACBQpQtWrVVNfWDfcWERFB3759+f33310dSrb74IMPKF68uH2ghaul9XfrlKlQkykNnMda4VWx3p2tgEsSheE+VG8UKEtISEBEuOuuu6hQoQIeHk6dM8twgQoVKvD0009z6dKlDDvjc6OSJUvSp08fV4fhNBklinK2EU+7uZEgkrjsK33+8FLwwh8w+X5XhWA4SFU5ffo0CQkJFCtWjKpVq+bazj7DqmfPvDmp2IABA1wdglNllCg8gaI4Vi4823hFFYUv9ppEkUNZLBYsFgteXl54eHhQrVo14uLiTAE/w3BjGSWKCFUdm22RGG4vOjqasLAw+/wVAMWKFcv0TljDMHK2jBKF+fpnOOT69eucOHHCPqzUw8ODxMTEVPdDGIbhnjJKFK2zLYpbcL1SFIxt6eowDKx9EJGRkZw8edJem6lixYqUK1fOdFYbRi6S7l+zqkal95orJXhfhb6Brg4jz7NYLCkK+JUoUYKAgADuuuuuLE8Snp6e1K1bl8DAQB566KEU9YhMmfEbnFVmfP/+/TRt2pQCBQowadKkdNupKTPuzNBd63ZvwHDVw9xwl70yuuHu6NGjunPnTo2Kikpxk1tWM2XGHeOsMuNnzpzRzZs366hRo3TixInptjNlxvNwmXHDAFs9pIUrUjxqrDtMyD9nKf3TNjy+XJnqdUcft8KUGc/+MuPlypUjNDQ001kFTZlxU2bcyMPi4uIICwtzdRimzLhNdpcZd5QpM56Hy4wbedf169eJjo5mz549WCwWtvt7U6lSJcqWLZut90SYMuMpZXeZcUeZMuN5u8y4kQetXbuWevXqcfHiRSwWC6VLlyYwMJBy5cpl+41zpsz4rcnqMuOOMmXG82iZ8Zyo8K4qUHZ65g2N23bt2jV69OjB3r178fLyok6dOtSsWTPTa9TOZsqMW2V3mXFHmTLjebDMeE59NPCqolpm2q0OAjAyYbFYUozgWbBggY4ePVr37NnjwqisTJnxlLK7zHhERIRWqlRJixUrpiVKlNBKlSppdHR0qnamzHgeLDOeUzXMV1W3lhwBkc+5OpRcY+/evQwaNIi2bdvy+uuvp3gtK8qMG3mDKTOee8uMu92lJyPrxMTEMGrUKEJCQli7di3z5s2zz11tGLcqeZnxvKZkyZL2IbW5kdslipjgE+ZsIgv8/PPPBAYGMm7cOBISEnjmmWfYsWMHBQoUcHVohhvr2bNnnpuLAqxlxr28cu8g0ty7Z0aarl69Sv/+/VmyZAkAwcHBzJo1i6ZNm7o4MsMwciq3O6Mw7kzhwoWJioqiSJEiTJo0iW3btpkkYRhGhswZRR6wdetWSpYsSe3atRER5s2bh6enJ1WrVnV1aIZhuAFzRpGLRUdHM3ToUBo1asSgQYPsN3fVqFHDJAnDMBxmEkUupKosXrwYX19fpk+fjoeHB/Xr13fbgmWmzLhry4wvXLiQ4OBggoODadasGTt37kyznaopM55r3e4NGK56BFUrovrZP7dxC0recPjwYW3fvr1inddcmzZtqjt37rzt9WVUZjy7mDLjjnFWmfF169bZy2n/9NNP2qhRozTbmTLjubfMuMs/+G/1Ye7MTt+lS5e0ZMmSCmjJkiV19uzZmpiYeEfrTP4LtzUOpzwyk/wD8KOPPtLBgwerquq8efO0T58+KdoePnxYK1eurKqqffr00Y8//jjT9V++fFn79++vgYGBGhQUpEuWLEm13W+++SbFndnDhw/Xli1b6vPPP6/VqlXTCxcu2NvWqlVLT58+rWfPntWHH35YGzZsqA0bNtS//vor1bavXbtm33byO7ODgoK0YMGCGhISomvWrEnxntOnT2vXrl01ODhYg4ODdd26dSnivXz5st5///1ar149DQwM1KVLl6qq6pUrV/TBBx/U4OBgDQgI0EWLFqmq6siRI9XPz0+DgoL0xRdfzPBYRUVFacWKFdN8rXfv3va7r1VVu3TpovXr11d/f3+dPXu2/fkiRYro66+/ro0aNdK1a9fqF198oaGhoRoSEqIDBw60J49BgwZpgwYN1N/fX0ePHp1hXLdi1apVGSaKdu3a2e92j4+PV29vb7VYLPrll1/qwIED7e0GDhyoX375paqqJiYmavXq1Z32xeFWZXWicGpntoh0AKYAnsA8VX3vptcfA0baFq8Ag1U17fNaI1PFihVj+PDhHD58mEmTJtmrcOYWpsy4lSvLjH/88cc88MADab5myoybMuO3TEQ8gRlAWyAc2CIiy1V1b7JmR4EWqnpBRB4A5gCNnRVTbhMZGcnLL79M69at6dOnDwCvv/6606q7NsjvmnIvpsx4Sq4qM75q1So+/vhj/vrrrzRfN2XGTZnx29EIOKyqR1T1OrAI6JK8gaquV9Wkr28bgcqZrTSh9BXo45/lwboTi8XCvHnzuPvuu/nss8947bXXiI+PB27tA9JdmDLjt8YZZcZ37drFU089xbJly+zVcW9myoybMuO3oxJwItlyuO259DwJpFlLWEQGishWEdl6vfIFmHx/FobpXnbv3s19993H008/zYULF2jTpg0rV650eQnw7GDKjFtld5nxsLAwHn74Yb744gvq1KmTblymzLgpM37rveTwCNZ+iaTlPsC0dNq2AvYB3pmt169+5p2fuVFMTIyOGDFCvby8FNDy5cvrl19+maKktjPktFFPqqbMeHaXGX/yySe1ZMmSGhISoiEhIdqgQYM04zJlxnNvmXFnJoqmwK/Jll8FXk2jXTDwL1DHkfXm1UQRGxurvr6+KiI6ZMiQFKNsnCknJArDPZw6dUrbtGnj6jBcYvLkyTpv3jxXh2HnTqOetgA+IlIDOAk8CvwneQMRqQr8D+ijqgedGItbCg8Pp3DhwpQuXZoCBQowf/58ABo3Nv39Rs6TvMx4XqsgW7JkSfuAktzIaX0UqpoAPAf8ivWy0tequkdEBonIIFuz0YA3MFNEdojI1nRWl6ckJCTwwQcf4Ofnl2JER+PGjU2SMHI0U2Y8d3LqnqnqT8BPNz03K9nPTwFPOTMGd7Np0yaeeeYZe5mE6OhoEhIScvUvoWEYOZvb1XoqeKg8tF7s6jCy3MWLFxkyZAhNmzZl586dVKtWje+//54lS5aYJGEYhku53SeQx7X8sCvS1WFkqQsXLuDv78/p06fx8vLixRdf5PXXX08xVt8wDMNV3C5R5EalSpXigQce4ODBg3z00UcEBQW5OiTDMAw7t7v0lBvExcUxduxY/vzzT/tz06dPZ82aNSZJ3Kbly5fz3nvvZd4wl5s/fz5ly5albt26+Pr6pipRPmfOHHx9ffH19aVRo0YpynHEx8fzyiuv4OPjQ2BgII0aNUr3RjdXev7551mzZo2rw0jXa6+9RpUqVTIt+z5u3Dhq167N3Xffza+//mp/Punu+dq1azNs2DD73fvTp0/n008/dWrs6brdcbWuegT45VPdcebWBxbnECtXrtQ6deoooH5+ftleZvlWpRqPXWZaykd6PvsnZbvhK50b6C2wWCx3XFX3Tjizwuinn36qzz77rKqqnjt3Tr29vTUsLExVb9x8GBkZqaqq27Zt0ypVqmhERISqWqvI9u3bV2NjY1XVehPZ4sWLszS+O/19P3/+vDZu3PiW3pPdFV03bNigp06dyrDs+549ezQ4OFhjY2P1yJEjWrNmTfuxCQ0N1fXr16vFYtEOHTroTz/9pKrWMvp169Z1KIasvo/C7c4oLIXiIcT9qqKePXuWPn360Lp1aw4ePIivry8zZ86017wx0nbs2DF8fX156qmnCAwM5LHHHmPFihXcc889+Pj4sHnzZsD6Tfq5554D4MyZM3Tr1o2QkBBCQkJYv349x44dw8/PjyFDhlC/fn1OnDjByy+/TGBgIEFBQSxenPYAic2bN9OsWTPq1atHs2bNOHDgAGAdqrxnzx57u5YtW7Jt2zauXr3KE088QWhoKPXq1WPZsmX2+B555BEeeugh2rVrx5UrV2jdujX169cnKCjI3g6s1Wx9fX1p27YtvXv3ZtKkSQD8+++/dOjQgQYNGtC8eXP279+f4bHz9vamdu3aREREADB+/HgmTpxImTJlAKhfvz79+vVjxowZxMTEMHfuXKZNm0aBAgUAawmTnj17plrvli1baNasGSEhITRq1IjLly+nOP4AnTp1YvXq1YB1QqXRo0fTuHFj3n333RTrXL16NQ899BAAv/32G02bNqV+/fo88sgjqarigrWkRvJ6VGPHjiU0NJTAwEAGDhxo//bdsmVLRo0aRYsWLZgyZQrbtm2jRYsWNGjQgPbt29uPydy5cwkNDSUkJITu3bsTExOT4TF1RJMmTahQoUKGbZYtW8ajjz5KgQIFqFGjBrVr12bz5s1ERERw6dIlmjZtiojQt29fli5dCljnu69evbr9dz5b3W6GcdXD3e7MTkxM1NmzZ9vniShYsKC+/fbbGhcX5+rQHOLqM4qjR4+qp6en7tq1SxMTE7V+/fo6YMAAtVgsunTpUu3SpYuqpvwm3bNnT/3ggw9U1foN9uLFi3r06FEVEXt5iCVLlmibNm00ISFBT58+rVWqVNFTp06l2n50dLT9G+nvv/+uDz/8sKpa78RNmiPh1KlT6uPjo6qqr776qn7xxReqqnrhwgX18fHRK1eu6KeffqqVKlXS8+fPq6r1W250dLSqWktu1KpVSy0Wi27ZskVDQkI0JiZGL126pLVr19aJEyeqqur999+vBw8eVFXVjRs3aqtWrVLFm/w4HD9+XENCQvTatWuqqlqqVCm9ePFiivZLly7Vbt266c6dOx36thoXF6c1atTQzZs3pzg+yberqtqxY0d7yQ7AfmYSHx+vVapU0StXrqiqdc6JL774QiMjI7V58+b259977z198803U22/b9++unz5cvty0vFUVX388cftr7Vo0cI+b8n169e1adOmevbsWVW1lgwZMGCAqlrPupK89tprOnXq1FTb/OOPP+zlS5I/mjZtmuGxyuiM4tlnn7X/nqiqPvHEE/rNN9/oli1btHXr1vbn16xZk2LujLffflsnTZqU4XZV3evObAPrfRCvvfYaFy9epH379syYMYNatWq5Oiy3UqNGDXvfTUBAAK1bt0ZECAoKSjFNZZK0ynBfuHCBatWq0aRJE8Ba3rt37954enpSvnx5WrRowZYtW+jcuXOKdUVHR9OvXz8OHTqEiNir9Pbs2ZO2bdvy5ptv8vXXX/PII48A1m/Fy5cvt58FxMbGEhYWBkDbtm0pXbo0kH4p8L/++osuXbrYq5Amfdu+cuUK69evt28HrH1daVm8eDGrVq3iwIEDzJ07l4IFC6Z7bFXTL9WelgMHDlChQgV7WXBHbq5LXlbcy8uLDh068P3339OjRw9+/PFHJkyYwJ9//snevXu55557ALh+/TpNmzZNta6IiAjKli1rX161ahUTJkwgJiaGqKgoAgIC7Mcsqdz7gQMH2L17t708fWJiov0b/+7du/m///s/Ll68yJUrV1IU+UvSqlWrNIsl3gnV2y9lntmZpDOYROEEV69excvLiwIFClCqVClmzZpFYmIijzzyiPuXAY98LvM2AH0DrY8skHQpBKwTxCQte3h43NI84MmHG6f1BwnWiYTmzp0LwE8//cTrr79Oq1at+O677zh27BgtW7YEoFKlSnh7e7Nr1y4WL15sn7BHVfn2229TVYDdtGlTiu0nLwWeL18+qlevTmxsbLpxWSwWSpYs6dAHVq9evZg+fTobNmygY8eOPPDAA9x11134+/uzbds27r//RvXl7du34+/vT+3atQkLC0s1X8bN0kssyUuMQ8qy7MnLiifFN2PGDEqXLk1oaCjFihVDVWnbti1fffVVhvtWqFAh+7pjY2MZMmQIW7dupUqVKowZMybFdpOOt6oSEBDAhg0bUq2vf//+LF26lJCQEObPn2+/XJbcqlWrGD58eKrnCxcuzPr16zOMNz3plSyvXLky4eHhqZ5P4qpS5m7XR5HTLV++HH9/fyZMmGB/rnv37vTs2dP9k4SbyKwMN1jLey9evJjExEQiIyNZs2YNjRo14tlnn7WXqq5YsSLR0dFUqmStjp9UayvJo48+yoQJE4iOjraf8bRv355p06bZP/D//vvvNGNMrxT4vffey/fff09sbCxXrlyxz0ZXvHhxatSowTfffANYP/yS7t5PT9OmTenTpw9TpkwBYMSIEYwcOZLz588D1rLn8+fPZ8iQIRQuXJgnn3ySYcOGcf36dcD67X3BggUp1unr68upU6fYsmULAJcvXyYhIYHq1auzY8cOLBYLJ06cyPA6esuWLdm+fTtz5861f+tv0qQJ69at4/DhwwDExMTYy8Mn5+fnZ2+TlBTKlCnDlStXWLJkSZrbu/vuu4mMjLQnivj4eHv/0uXLl6lQoQLx8fFpzpQHN84obn7cbpIAaynzRYsWERcXx9GjRzl06BCNGjWiQoUKFCtWjI0bN6KqfP7553TpcmMan4MHDxIYmDVfwG6F2yUKj2v5YOdZV4eRSlhYGF27dqVLly6EhYXx66+/pviGZWSfKVOmsGrVKoKCgmjQoEGKTuck3bp1Izg4mJCQEO6//34mTJjAXXfdlardiBEjePXVV7nnnntITExM8VqPHj1YtGhRis7Z119/nfj4eIKDgwkMDOT1119PM8bHHnuMrVu30rBhQxYuXIivry8AoaGhdO7cmZCQEB5++GEaNmxon8Fu4cKFfPzxx4SEhBAQEJCiAzw9I0eO5NNPP+Xy5ct07tyZJ554gmbNmuHr68vTTz/NggUL7Jdh3n77bcqWLYu/vz+BgYF07do1xWUegPz587N48WKGDh1KSEgIbdu2JTY2lnvuucd+ifCll16ifv366cbk6elJp06d+Pnnn+0z6pUtW5b58+fTu3dvgoODadKkSZqXWDp27Gj/1l+yZEmefvppgoKC6Nq1q/1y2M3y58/PkiVLGDlyJCEhIdStW9f+If/WW2/RuHFj2rZta/8/uFMjRoygcuXKxMTEULlyZcaMGQNYv0SOHj0asF5C7dmzJ/7+/nTo0IEZM2bYz7o++ugjnnrqKWrXrk2tWrVSTD27bt062rRpkyVx3pLb7dxw1aOBV5WMO1Gz2fXr13XixIlauHBhBbRYsWI6ZcqUHD/s1VGmzHj2u3z5sqpah0M2aNBAt23b5uKIcpZ77rkn28rs5yTbt2/Xxx9/3KG2pjM7Bzl37px90newzqP8wQcf2C9VGMbtGDhwIHv37iU2NpZ+/fpl+O08L3r//fcJCwujZMmSrg4lW507d4633nrLJds2ieIOeHt7U6ZMGWrUqMH06dN58MEHXR2SkQt8+eWXrg4hR8urpfaTRm25gtslCkuh6xBcNvOGTqCqLFy4kEaNGlGnTh1EhAULFlCiRAkKFy7skpgMwzCcze06s2N9zsDKXtm+3QMHDtCmTRv69OnDkCFD7KNaKlSoYJKEYRi5mtsliuwWGxvLG2+8QXBwMH/88Qfe3t48/vjjrg7LMAwj27jdpafstGLFCgYPHmwft/3EE08wYcIEvL29XRyZYRhG9jFnFOk4c+YMnTp14vDhw/j7+7NmzRo+/vhjkyQMt3Hs2DEKFSpE3bp18ff3p2/fvvYSJGAtY9KoUSN72fE5c+akeP/nn39OYGAgAQEB+Pv728uS5CRLly5l7Nixrg4jXd988w0BAQF4eHiwdevWdNv98ssv3H333dSuXTtFufyoqCjatm2Lj48Pbdu25cKFCwD8888/9O/f39nh33C742pd9XBmUcDExES1WCz25fHjx+u4cePcpoCfM9w8HhvGpHikZ/bsrSnaPf308nTbupor73lxZsnzo0ePakBAgKpa97FVq1a6YMECVVWNiIjQKlWq2O/RiIyM1Pr16+sPP/ygqqo//fST1qtXT0+ePKmqqteuXdM5c+ZkaXxZUf67adOm9rLp2bXNW7F3717dv3+/tmjRQrds2ZJmm4SEBK1Zs6b++++/GhcXp8HBwbpnzx5VVX355Zd13Lhxqqo6btw4HTFihP19rVu31uPHj6e73ZuRl8qMO8uOHTto1qxZipIFI0aM4JVXXiF//vwujCxvc7TMeHrlwBMTE3nppZcICgoiODiYadOmAVC9enXGjh3LvffeyzfffMNXX31FUFAQgYGBjBw5Ms1Y0isNPnLkSGbOnGlvN2bMGN5//30AJk6cSGhoKMHBwbzxxhv2fbq55PngwYNp2LAhAQEB9nZgrTfl6+vLvffey7Bhw+x3MqdXzjw9np6eNGrUiJMnTwLWmlb9+/e336NRpkwZJkyYYP82O27cOCZNmmSvM1SwYEGefvrpVOtNr6R78jITkyZNst+dnLz89zvvvEP16tXtFQxiYmKoUqUK8fHxDpVUP3jwIAUKFLCXTf/+++9p3Lgx9erVo02bNpw5c8b+/zFw4EDatWtH3759iYyMpHv37oSGhhIaGsq6deuA9H+H7oSfn1+qul8327x5M7Vr16ZmzZrkz5+fRx991P7/uWzZMvr16wdAv3797CXHwVowctGiRXcco0NuN8O46hFSrlSWToJz6dIlHT58uHp4eCigdevWTXFWkde5+ozC0TLj6ZUDnzlzpj788MP215LKUlerVk3Hjx+vqqonT57UKlWq6NmzZzU+Pl5btWql3333XapY0isNvn37dr3vvvvs7fz8/PT48eP666+/6tNPP20/a+jYsaP++eefqUqeJ48rISFBW7RooTt37tRr165p5cqV9ciRI6qq+uijj9pLTqdXzvzmY5d0RnHt2jVt2bKl7ty5U1VVu3XrpkuXLk3R/uLFi1qqVClVTbskeVrSK+metF1V1YkTJ+obb7yhqinLf6uqdu7cWf/44w9VtZb/fvLJJ1XVsZLqn3zyib7wwgv25aioKPvf7ty5c+2vvfHGG1q/fn2NiYlRVdXevXvr2rVrVdVait3X11dV0/8dSu7SpUtplhwPCQmxnwWkJaMzim+++ca+36qqn3/+ub1ke4kSJVK0LVmypP3nv/76Szt16pTmOvP8ndleUUXhi70w+f7MG2dAVVm6dCnDhg0jPDwcDw8P/vvf/zJ27FhTvC+HcaTMeHrlwFesWMGgQYPw8rL+qieV+YYbZai3bNlCy5Yt7XWNHnvsMdasWUPXrl1TxKGadmnwevXqcfbsWU6dOkVkZCSlSpWiatWqTJ06ld9++4169eoB1jOSQ4cOUbVq1RQlzwG+/vpr5syZQ0JCAhEREezduxeLxULNmjWpUaMGAL1797b3I6RXztzPzy9FzP/++y9169bl0KFD9OjRg+DgYPu+pPV7fqu/++mVdM9I0nFP+nnx4sW0atWKRYsWMWTIEIdLqt9ccjw8PJxevXoRERHB9evX7ccNrEX4kqqurlixgr1799pfu3TpEpcvX073dyi5YsWKZVvJ8cyUK1eOU6dOZWks6XG7RJEVzp07x4ABA/jhhx8AaNiwIbNnzzalEhyg+kbmjYCBAxswcGCDLNmmI2XG0ysHnt4HIqQsQ52WTZs28cwzzwDWmdSioqLSLA0O1gKBS5Ys4fTp0zz66KP29b766qv2dSQ5duxYipLjR48eZdKkSWzZsoVSpUrRv3//DEuOJ607rXLmN6tVqxY7duwgIiKCli1bsnz5cjp37kxAQABbt25NMf/Gtm3b8Pf3B6wJ+eaS5I7KqOQ4pCz33rlzZ1599VWioqLs27t69apDJdULFSpEdHS0fXno0KG88MILdO7cmdWrV9svd928TYvFwoYNG1KV6x46dGiav0PJXb58mebNm6cZz5dffmk/frcivZLjYJ1lMCIiggoVKhAREUG5cjdm98zOkuN5so+iWLFiHD58mOLFizN9+nQ2btxokoSbS68ceLt27Zg1a5Y9oURFRaV6b+PGjfnzzz85d+4ciYmJfPXVV7Ro0YLGjRvbS0p37tw53dLgYC05vmjRIpYsWUKPHj0Aa8nxTz75xD6l58mTJzl7NnXl40uXLlGkSBFKlCjBmTNn+PnnnwFrSe8jR47Yz5qST9fqaDnzJBUqVOC9995j3LhxADz77LPMnz/f/mF8/vx5Ro4cyYgRIwB49dVXGTFiBKdPnwas3+inTp2aar1plXQvX748Z8+e5fz588TFxdm/kKWlaNGiNGrUiP/+97906tQJT09Ph0uqJy85Dil/Bz777LN0t9muXTumT59uX046BhmVlE+SdEaR1uN2kgRYKwYfOnSIo0ePcv36dRYtWmRP4J07d7bvy2effeaykuNulyiuV4qC91ve8vvWrVtnr8NfoEABFi1axP79+3n22WfNvNW5QHrlwJ966imqVq1qLymeVh2lChUqMG7cOFq1akVISAj169dP8QeZJL3S4GD9Bn758mUqVapkL9vdrl07/vOf/9C0aVOCgoLo0aMHly9fTrXekJAQ6tWrR0BAAE888YR9lrdChQoxc+ZMOnTowL333kv58uXtJccdLWeeXNeuXYmJiWHt2rVUqFCBBQsW8PTTT+Pr60uzZs144okn7LPDPfjggzz77LO0adOGgIAAGjRokOYkUWmVdM+XL599juxOnTplWr67V69eLFiwIMUlKUdKqt933338/fff9mQ5ZswYHnnkEZo3b27v4E7L1KlT2bp1K8HBwfj7+zNr1iwg45Lyt+u7776jcuXK9kmkkmbQO3XqlL02nJeXF9OnT6d9+/b4+fnRs2dPAgICAHjllVf4/fff8fHx4ffff+eVV16xr3vVqlV07NgxS+LM1O12brjqcavDY8+dO6dPPfWUAik6jAzHmDLjrpVUctxisejgwYN18uTJLo4oZxk2bJj+/vvvrg4j28XGxmrjxo3THe5rhsc6SFX57LPP8PX1Zd68eeTLl4+KFStmeN3XMHKauXPnUrduXQICAoiOjk7V35HXjRo1ipiYGFeHke3CwsJ477337IM0nE3c7YPTv4Ho3m0Zx7x//34GDRrEn3/+CVjHbn/00UdZNoNVXrJv375UI2kMw8jZ0vq7FZFtqtrwdtaX60Y9hYeHExISwvXr1ylTpgzvv/8+ffr0MUNe74BmMHLIMIycxRlf/nNdoqhcuTJ9+vTBw8OD9957L8W4eePWFSxYkPPnz+Pt7W2ShWHkcKrK+fPnKViwYJau1+0vPUVERDB8+HAGDRpkH/dssVjw8Mi13S/ZKj4+nvDw8FRj4Q3DyJkKFixI5cqVyZcvX4rn89Slp8K7qkDZ6SSeHsxHH33Ea6+9xqVLlzh8+DBbtmxBREySyEL58uVLcYerYRh5j1M/UUWkg4gcEJHDIvJKGq+LiEy1vb5LRBy66217wgmaNGnC0KFDuXTpEg899BDffvutuTRiGIbhBE679CQinsBBoC0QDmwBeqvq3mRtHgSGAg8CjYEpqprhzOnlPYrpOb2KBaVy5cpMmzaNLl26mCRhGIaRgTu59OTMM4pGwGFVPaKq14FFwM23u3YBPrfdD7IRKCkiFTJaaZTGIAgvvPAC+/bto2vXriZJGIZhOJEz+ygqASeSLYdjPWvIrE0lICJ5IxEZCAy0LcYBuydPnszkyZOzNGA3VAY45+ogcghzLG4wx+IGcyxuyLiCZAacmSjS+pp/83UuR9qgqnOAOQAisvV2T59yG3MsbjDH4gZzLG4wx+IGEUl/LtZMOPPSUzhQJdlyZeDm4umOtDEMwzBcyJmJYgvgIyI1RCQ/8Ciw/KY2y4G+ttFPTYBoVY24eUWGYRiG6zjt0pOqJojIc8CvgCfwiaruEZFBttdnAT9hHfF0GIgBBjiw6jlOCtkdmWNxgzkWN5hjcYM5Fjfc9rFwuzuzDcMwjOxlbmE2DMMwMmQShWEYhpGhHJsonFX+wx05cCwesx2DXSKyXkRCXBFndsjsWCRrFyoiiSLSIzvjy06OHAsRaSkiO0Rkj4j8md0xZhcH/kZKiMj3IrLTdiwc6Q91OyLyiYicFZHd6bx+e5+btzs1njMfWDu//wVqAvmBnYD/TW0eBH7Gei9GE2CTq+N24bFoBpSy/fxAXj4Wydr9gXWwRA9Xx+3C34uSwF6gqm25nKvjduGxGAWMt/1cFogC8rs6dicci/uA+sDudF6/rc/NnHpG4ZTyH24q02OhqutV9YJtcSPW+1FyI0d+L8BaP+xb4Gx2BpfNHDkW/wH+p6phAKqaW4+HI8dCgWJirfdTFGuiSMjeMJ1PVddg3bf03NbnZk5NFOmV9rjVNrnBre7nk1i/MeRGmR4LEakEdANmZWNcruDI70UdoJSIrBaRbSLSN9uiy16OHIvpgB/WG3r/Af6rqpbsCS9Hua3PzZw6H0WWlf/IBRzeTxFphTVR3OvUiFzHkWPxITBSVRNzebFIR46FF9AAaA0UAjaIyEZVPejs4LKZI8eiPbADuB+oBfwuImtV9ZKTY8tpbutzM6cmClP+4waH9lNEgoF5wAOqej6bYstujhyLhsAiW5IoAzwoIgmqujRbIsw+jv6NnFPVq8BVEVkDhGAt/5+bOHIsBgDvqfVC/WEROQr4ApuzJ8Qc47Y+N3PqpSdT/uOGTI+FiFQF/gf0yYXfFpPL9Fioag1Vra6q1YElwJBcmCTAsb+RZUBzEfESkcJYqzfvy+Y4s4MjxyIM65kVIlIeayXVI9kaZc5wW5+bOfKMQp1X/sPtOHgsRgPewEzbN+kEzYUVMx08FnmCI8dCVfeJyC/ALsACzFPVNIdNujMHfy/eAuaLyD9YL7+MVNVcV35cRL4CWgJlRCQceAPIB3f2uWlKeBiGYRgZyqmXngzDMIwcwiQKwzAMI0MmURiGYRgZMonCMAzDyJBJFIZhGEaGTKIwciRb5dcdyR7VM2h7JQu2N19Ejtq2tV1Emt7GOuaJiL/t51E3vbb+TmO0rSfpuOy2VUMtmUn7uiLyYFZs28i7zPBYI0cSkSuqWjSr22awjvnAD6q6RETaAZNUNfgO1nfHMWW2XhH5DDioqu9k0L4/0FBVn8vqWIy8w5xRGG5BRIqKyErbt/1/RCRV1VgRqSAia5J9425ue76diGywvfcbEcnsA3wNUNv23hds69otIs/bnisiIj/a5jbYLSK9bM+vFpGGIvIeUMgWx0Lba1ds/y5O/g3fdibTXUQ8RWSiiGwR6zwBzzhwWDZgK+gmIo3EOhfJ37Z/77bdpTwW6GWLpZct9k9s2/k7reNoGKm4un66eZhHWg8gEWsRtx3Ad1irCBS3vVYG652lSWfEV2z/vgi8ZvvZEyhma7sGKGJ7fiQwOo3tzcc2dwXwCLAJa0G9f4AiWEtT7wHqAd2BucneW8L272qs397tMSVrkxRjN+Az28/5sVbyLAQMBP7P9nwBYCtQI404ryTbv2+ADrbl4oCX7ec2wLe2n/sD05O9/13gcdvPJbHWfSri6v9v88jZjxxZwsMwgGuqWjdpQUTyAe+KyH1Yy1FUAsoDp5O9Zwvwia3tUlXdISItAH9gna28SX6s38TTMlFE/g+IxFqFtzXwnVqL6iEi/wOaA78Ak0RkPNbLVWtvYb9+BqaKSAGgA7BGVa/ZLncFy40Z+UoAPsDRm95fSER2ANWBbcDvydp/JiI+WKuB5ktn++2AziLykm25IFCV3FkDysgiJlEY7uIxrDOTNVDVeBE5hvVDzk5V19gSSUfgCxGZCFwAflfV3g5s42VVXZK0ICJt0mqkqgdFpAHWmjnjROQ3VR3ryE6oaqyIrMZa9roX8FXS5oChqvprJqu4pqp1RaQE8APwLDAVay2jVarazdbxvzqd9wvQXVUPOBKvYYDpozDcRwngrC1JtAKq3dxARKrZ2swFPsY6JeRG4B4RSepzKCwidRzc5hqgq+09RbBeNlorIhWBGFVdAEyybedm8bYzm7QswlqMrTnWQnbY/h2c9B4RqWPbZppUNRoYBrxke08J4KTt5f7Jml7Gegkuya/AULGdXolIvfS2YRhJTKIw3MVCoKGIbMV6drE/jTYtgR0i8jfWfoQpqhqJ9YPzKxHZhTVx+DqyQVXdjrXvYjPWPot5qvo3EARstl0Ceg14O423zwF2JXVm3+Q3rHMbr1Dr1J1gnUtkL7BdRHYDs8nkjN8Wy06sZbUnYD27WYe1/yLJKsA/qTMb65lHPltsu23LhpEhMzzWMAzDyJA5ozAMwzAyZBKFYRiGkSGTKAzDMIwMmURhGIZhZMgkCsMwDCNDJlEYhmEYGTKJwjAMw8jQ/wOUmLldgsPNXgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "skplt.metrics.plot_roc(TEST_GENERATOR.classes.tolist(), Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(file):\n",
    "    img_path = test_directory + 'muzzle/'\n",
    "    img = image.load_img(img_path + file, target_size=(225, 225))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
    "    return tf.keras.applications.mobilenet.preprocess_input(img_array_expanded_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_directory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gmlkd\\FastCampus\\mini_project\\개물림 사고 예방\\Classification\\training_mobilenet.ipynb 셀 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gmlkd/FastCampus/mini_project/%EA%B0%9C%EB%AC%BC%EB%A6%BC%20%EC%82%AC%EA%B3%A0%20%EC%98%88%EB%B0%A9/Classification/training_mobilenet.ipynb#ch0000015?line=0'>1</a>\u001b[0m preprocessed_image \u001b[39m=\u001b[39m prepare_image(\u001b[39m'\u001b[39;49m\u001b[39mPCM20181227000209990_P2.jpg\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gmlkd/FastCampus/mini_project/%EA%B0%9C%EB%AC%BC%EB%A6%BC%20%EC%82%AC%EA%B3%A0%20%EC%98%88%EB%B0%A9/Classification/training_mobilenet.ipynb#ch0000015?line=1'>2</a>\u001b[0m predictions \u001b[39m=\u001b[39m DeepLearning\u001b[39m.\u001b[39mpredict(preprocessed_image)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gmlkd/FastCampus/mini_project/%EA%B0%9C%EB%AC%BC%EB%A6%BC%20%EC%82%AC%EA%B3%A0%20%EC%98%88%EB%B0%A9/Classification/training_mobilenet.ipynb#ch0000015?line=2'>3</a>\u001b[0m predictions\n",
      "\u001b[1;32mc:\\Users\\gmlkd\\FastCampus\\mini_project\\개물림 사고 예방\\Classification\\training_mobilenet.ipynb 셀 16\u001b[0m in \u001b[0;36mprepare_image\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gmlkd/FastCampus/mini_project/%EA%B0%9C%EB%AC%BC%EB%A6%BC%20%EC%82%AC%EA%B3%A0%20%EC%98%88%EB%B0%A9/Classification/training_mobilenet.ipynb#ch0000015?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_image\u001b[39m(file):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gmlkd/FastCampus/mini_project/%EA%B0%9C%EB%AC%BC%EB%A6%BC%20%EC%82%AC%EA%B3%A0%20%EC%98%88%EB%B0%A9/Classification/training_mobilenet.ipynb#ch0000015?line=1'>2</a>\u001b[0m     img_path \u001b[39m=\u001b[39m test_directory \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmuzzle/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gmlkd/FastCampus/mini_project/%EA%B0%9C%EB%AC%BC%EB%A6%BC%20%EC%82%AC%EA%B3%A0%20%EC%98%88%EB%B0%A9/Classification/training_mobilenet.ipynb#ch0000015?line=2'>3</a>\u001b[0m     img \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mload_img(img_path \u001b[39m+\u001b[39m file, target_size\u001b[39m=\u001b[39m(\u001b[39m225\u001b[39m, \u001b[39m225\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gmlkd/FastCampus/mini_project/%EA%B0%9C%EB%AC%BC%EB%A6%BC%20%EC%82%AC%EA%B3%A0%20%EC%98%88%EB%B0%A9/Classification/training_mobilenet.ipynb#ch0000015?line=3'>4</a>\u001b[0m     img_array \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mimg_to_array(img)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_directory' is not defined"
     ]
    }
   ],
   "source": [
    "preprocessed_image = prepare_image('PCM20181227000209990_P2.jpg')\n",
    "predictions = DeepLearning.predict(preprocessed_image)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensor2.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71fd5fe6be37b4db0cd97162c0729102804300edc330f59999a435bca5ce69d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
